{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RogerGMarch/Stance_Detection_SemEval_2017/blob/main/Transfer_Learning_for_Sentiment_Classification_Twiiter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMpBzDCvOFTh"
      },
      "source": [
        "# Transfer Learning\n",
        "\n",
        "In this colab, we will explore sentiment classification: in particular, we'll see how we can train an \"adapter\" over BERT embeddings, learning how the semantic information captured by BERT can be used to make predictions about the sentiment (positive or negative) expressed by a sentence.\n",
        "\n",
        "*IMPORTANT*:\n",
        "\n",
        "You will have to run this notebook on a GPU. To activate GPU-usage:\n",
        "1. click 'Runtime'\n",
        "2. click 'Change runtime type'\n",
        "3. Select 'GPU' for 'Hardware accelerator' and save"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWUJGx6FH1ef"
      },
      "source": [
        "## 0. Import packages\n",
        "\n",
        "First, run this cell to import all necessary packages (and install them, if necessary). Then, continue with the following cell to load the required data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6357789bfe1c4603ab378c7aa3fc813e",
            "af008c8cc7e6429f89062fdfc23604ab",
            "cbd4a6dc07cb498abb1dce78e6f83ebb",
            "7599bde5999e44eea1b9d481daaf59f0",
            "cd7430b9b9fb4f298a3eb54fb36e77f2",
            "6aa03249330f4ac88e905dcbaef56afb",
            "c250c89260b946579b805c726a84513b",
            "260c26a28a914361bfac3fe297a28338",
            "cb16cbb90c36423f877dcf2d2eb0f416",
            "10b59045e2cc4b80970a3fff1338911b",
            "7d59387fd9f5409daf975079e15325bc",
            "99dd8548fc3d4616bb434f4f7efdc5e5",
            "32e3db7e36b9466590253acc8ef4ba45",
            "dbb14678393a4495a6286b24f37531f6",
            "1d32bbded75e47bb96789670e524d8ff",
            "be19a58554014e8f938da4776af6d3d6",
            "36a490f525a54034963d4ae63182ad5b",
            "269ad6d98bab42bb86ff51f9b59ae578",
            "ee611c4a852a46cd983f530db58916a1",
            "60da43d24f3c4eed9d6559aa2eb30c15",
            "65315d6c6dc44c27aad93e0dac18c196",
            "37179af5f9404a5f9ec7a0a15209f5a5",
            "6f177af253d341d8afe8fda96f5c305e",
            "a3dbe0afd3b942bfbafe747ea09805df",
            "2e71672af6ef449a9ff3881dc83d6412",
            "628e3504f1e249509b979c08b6e2ab84",
            "db935c1565ec498bb8167fd9eba8a26d",
            "bfe863c8e37b4dffa0c464c1f16133d6",
            "72d1c90f8cee4ed881ee898401caf178",
            "5a8569324b8348dd8f81dfad2750ea9e",
            "23d70222df9543ea94a2601f748375c4",
            "8e18c0cbd5cc45f58f8a22bad0b176ce",
            "a035eb6f0dbf4083a1c1890dd6019a51"
          ]
        },
        "id": "Z6cYZkMtPFqw",
        "outputId": "7f3a0a82-253b-4d81-b7be-9f0e5c82c833"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
            "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.35.2\n",
            "Uninstalling transformers-4.35.2:\n",
            "  Would remove:\n",
            "    /usr/local/bin/transformers-cli\n",
            "    /usr/local/lib/python3.10/dist-packages/transformers-4.35.2.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/transformers/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled transformers-4.35.2\n",
            "\u001b[33mWARNING: Skipping adapter-transformers as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting adapter-transformers\n",
            "  Using cached adapter_transformers-3.2.1.post0-py3-none-any.whl (6.4 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from adapter-transformers) (3.13.1)\n",
            "Collecting huggingface-hub<0.14.0,>=0.11.0 (from adapter-transformers)\n",
            "  Using cached huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from adapter-transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from adapter-transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from adapter-transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from adapter-transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from adapter-transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from adapter-transformers)\n",
            "  Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from adapter-transformers) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.14.0,>=0.11.0->adapter-transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->adapter-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->adapter-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->adapter-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->adapter-transformers) (2023.11.17)\n",
            "Installing collected packages: tokenizers, huggingface-hub, adapter-transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.19.4\n",
            "    Uninstalling huggingface-hub-0.19.4:\n",
            "      Successfully uninstalled huggingface-hub-0.19.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 2.16.0 requires huggingface-hub>=0.19.4, but you have huggingface-hub 0.13.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed adapter-transformers-3.2.1.post0 huggingface-hub-0.13.4 tokenizers-0.13.3\n",
            "Collecting huggingface\n",
            "  Downloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: huggingface\n",
            "Successfully installed huggingface-0.0.1\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Collecting huggingface-hub>=0.19.4 (from datasets)\n",
            "  Downloading huggingface_hub-0.20.1-py3-none-any.whl (330 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: huggingface-hub\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.13.4\n",
            "    Uninstalling huggingface-hub-0.13.4:\n",
            "      Successfully uninstalled huggingface-hub-0.13.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "adapter-transformers 3.2.1.post0 requires huggingface-hub<0.14.0,>=0.11.0, but you have huggingface-hub 0.20.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.20.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/adapters/__init__.py:27: FutureWarning: The `adapter-transformers` package is deprecated and replaced by the `adapters` package. See https://docs.adapterhub.ml/transitioning.html.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/843k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6357789bfe1c4603ab378c7aa3fc813e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/558 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99dd8548fc3d4616bb434f4f7efdc5e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertweetTokenizer'. \n",
            "The class this function is called from is 'BertTokenizer'.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/543M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f177af253d341d8afe8fda96f5c305e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing BertModel: ['roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'lm_head.decoder.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.pooler.dense.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.6.output.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.embeddings.position_ids', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.7.output.dense.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.0.intermediate.dense.bias', 'lm_head.bias', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'lm_head.dense.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.11.output.dense.bias', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.5.attention.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.3.attention.output.dense.bias', 'pooler.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.8.output.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# INSTALL MISSING PACKAGES\n",
        "from importlib.util import find_spec\n",
        "import pip\n",
        "\n",
        "required_packages = ['torch', 'datasets', 'pandas']\n",
        "\n",
        "for package in required_packages:\n",
        "  if find_spec(package) is None:\n",
        "    print(f'Installing package: {package}...')\n",
        "    pip.main(['install', package])\n",
        "\n",
        "!pip uninstall transformers\n",
        "!pip uninstall adapter-transformers\n",
        "!pip install -U adapter-transformers\n",
        "!pip install huggingface\n",
        "!pip install --upgrade datasets\n",
        "\n",
        "# IMPORT PACKAGES\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import datasets\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from transformers import pipeline\n",
        "from transformers import AutoConfig, AutoTokenizer, AutoModelWithHeads, AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, AdapterTrainer, EvalPrediction\n",
        "\n",
        "from transformers import TextClassificationPipeline\n",
        "from transformers import TrainerCallback\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('vinai/bertweet-base')\n",
        "model = BertModel.from_pretrained('vinai/bertweet-base',\n",
        "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
        "                                  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU_Z1REdCc4k"
      },
      "source": [
        "## 1. Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xa4yFAdqCXK"
      },
      "source": [
        "Do you remember which is the difference between: **training set**, **validation set** and **test set**?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDOevhWsHI1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a6ae0bf-fede-4fdb-b7d8-80ac2473a1d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/MyDrive'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5JUIM53RGGI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "c0e6d18e-48c6-4d93-f20f-3823535045f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'train_data = \"training_set_tweet.csv\"\\ntest_data = \"test_set_tweet.csv\"\\n\\ndf_train = pd.read_csv(train_data)\\ndf_test = pd.read_csv(test_data)\\n\\n\\n\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\n# Split the training data into training and validation sets\\n# Let\\'s assume we want to split the data into 80% training and 20% validation\\ntrain_ratio = 0.8\\ndf_train, df_val = train_test_split(df_train, test_size=1-train_ratio)\\n\\n# Now df_train_split and df_val_split are your new training and validation sets\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "\"\"\"train_data = \"training_set_tweet.csv\"\n",
        "test_data = \"test_set_tweet.csv\"\n",
        "\n",
        "df_train = pd.read_csv(train_data)\n",
        "df_test = pd.read_csv(test_data)\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Split the training data into training and validation sets\n",
        "# Let's assume we want to split the data into 80% training and 20% validation\n",
        "train_ratio = 0.8\n",
        "df_train, df_val = train_test_split(df_train, test_size=1-train_ratio)\n",
        "\n",
        "# Now df_train_split and df_val_split are your new training and validation sets\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = \"/content/drive/Shareddrives/Computational Semantics/Assignmen 3/Graded part/Scripts/data/train_clean_v4.json\"\n",
        "test_data = \"/content/drive/Shareddrives/Computational Semantics/Assignmen 3/Graded part/Scripts/data/test_clean_v4.json\"\n",
        "\n",
        "#train_data = \"./train_clean_v4.json\"\n",
        "#test_data = \"./test_clean_v4.json\"\n",
        "\n",
        "df_train = pd.read_json(train_data, lines=True)\n",
        "df_test = pd.read_json(test_data, lines=True)\n",
        "\n",
        "df_train = df_train.dropna()\n",
        "df_test = df_test.dropna()\n",
        "\n",
        "print(df_train['input'].iloc[0])\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Split the training data into training and validation sets\n",
        "# Let's assume we want to split the data into 80% training and 20% validation\n",
        "train_ratio = 0.8\n",
        "df_train, df_val = train_test_split(df_train, test_size=1-train_ratio)\n",
        "\n",
        "# Now df_train_split and df_val_split are your new training and validation sets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHe9LJqXH4Bu",
        "outputId": "9fb0004e-d0d7-44bd-eacf-77a7cae93d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@USER @USER @USER this shows michael paid for cigarillos. no robbery, no crime! pls rt HTTPURL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "AwT-YkMCEWE9",
        "outputId": "8d78b340-c9ff-4ca6-f0b4-fb534acc9ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  input   output\n",
              "1091  @USER @USER @USER @USER confused how an unarme...     deny\n",
              "1564                                           @USER ok  comment\n",
              "3395  @USER @USER keep rising up against them #paris...  comment\n",
              "3858  @USER @USER 'darren wilson, #fergusonmo. cop w...  support\n",
              "1863  @USER @USER what more hatred u want after abus...  comment\n",
              "...                                                 ...      ...\n",
              "3752                            @USER joy joy joy @USER  comment\n",
              "1079                                   @USER @USER wow.  comment\n",
              "3294  @USER god's miracles are just inexplicable,who...  support\n",
              "2217  @USER wait a min, i thought they said he was a...  comment\n",
              "2140  @USER raises hand oooh oooh i know, i know!......  comment\n",
              "\n",
              "[3390 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a7cb06bb-a489-4d0e-9fed-90983cb7c647\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1091</th>\n",
              "      <td>@USER @USER @USER @USER confused how an unarme...</td>\n",
              "      <td>deny</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1564</th>\n",
              "      <td>@USER ok</td>\n",
              "      <td>comment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3395</th>\n",
              "      <td>@USER @USER keep rising up against them #paris...</td>\n",
              "      <td>comment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3858</th>\n",
              "      <td>@USER @USER 'darren wilson, #fergusonmo. cop w...</td>\n",
              "      <td>support</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1863</th>\n",
              "      <td>@USER @USER what more hatred u want after abus...</td>\n",
              "      <td>comment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3752</th>\n",
              "      <td>@USER joy joy joy @USER</td>\n",
              "      <td>comment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1079</th>\n",
              "      <td>@USER @USER wow.</td>\n",
              "      <td>comment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3294</th>\n",
              "      <td>@USER god's miracles are just inexplicable,who...</td>\n",
              "      <td>support</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2217</th>\n",
              "      <td>@USER wait a min, i thought they said he was a...</td>\n",
              "      <td>comment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2140</th>\n",
              "      <td>@USER raises hand oooh oooh i know, i know!......</td>\n",
              "      <td>comment</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3390 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7cb06bb-a489-4d0e-9fed-90983cb7c647')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a7cb06bb-a489-4d0e-9fed-90983cb7c647 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a7cb06bb-a489-4d0e-9fed-90983cb7c647');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e89360a9-d45c-454f-945e-26bb6647cfe6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e89360a9-d45c-454f-945e-26bb6647cfe6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e89360a9-d45c-454f-945e-26bb6647cfe6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRFiXB3LfkWp"
      },
      "source": [
        "Let's inspect the dataset we are working with. Do you understand its structure?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeLD6X0Yfn6a"
      },
      "outputs": [],
      "source": [
        "df_train = df_train.dropna()\n",
        "df_test = df_test.dropna()\n",
        "df_val = df_test.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcDB6iZgCSPd"
      },
      "source": [
        "**Exercise:**\n",
        "Check whether our dataset is balanced (i.e. the number of items in the two classes is approximately the same). Why does this matter?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBQ9idgZCVzj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "020718de-d9ca-4bf4-ed3c-b3f07776ae5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'#breakingnews @USER germanwings airbus a320 crashes in french alps near digne HTTPURL #breaking #planecrash'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "df_test['input'].iloc[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrMRTyKYf5Bn"
      },
      "source": [
        "    Now we transform the dataset \"sentiment\" column in order to make it readable by our model during the training phase. In particular, we map classes to numbers: \"pos\" will be 1 and \"neg\" will be 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-1tg-0xsUk5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e09afdc9-6275-4582-dce5-02de77e1a9d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    2182\n",
            "1     668\n",
            "2     274\n",
            "3     266\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "label_mapping = {'comment': 0, 'support': 1, 'deny': 2, 'query': 3}\n",
        "\n",
        "df_train['label'] = df_train['output'].map(label_mapping)\n",
        "df_val['label'] = df_val['output'].map(label_mapping)\n",
        "df_test['label'] = df_test['output'].map(label_mapping)\n",
        "\n",
        "\n",
        "\n",
        "# e.g.:\n",
        "print(df_train['label'].value_counts())\n",
        "\n",
        "# Convert the pandas DataFrames to Hugging Face Dataset objects\n",
        "train_dataset = Dataset.from_pandas(df_train)\n",
        "validation_dataset = Dataset.from_pandas(df_val)\n",
        "test_dataset = Dataset.from_pandas(df_test)\n",
        "\n",
        "# Create a DatasetDict\n",
        "dataset = DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'validation': validation_dataset,\n",
        "    'test': test_dataset\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6stjY8Ea1PUx"
      },
      "source": [
        "## 2. Training an adapter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtGHC_ZRzckM"
      },
      "source": [
        "The idea of **adapters** in Transformers comes from Houlsby et al. (2019) in the paper “[Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/pdf/1902.00751.pdf)”.\n",
        "\n",
        "# Fine-tuning vs. Adapters\n",
        "\n",
        "Fine-tuning involves updating the parameters of some or all layers of the pre-trained model to adapt it to the new task. It tends to work very well; at the same time, it is notoriously hard to fine tune Large Language Models (LLMs) for a specific task, and it can be problematic:\n",
        "- Given their enormous size (e.g. GPT3 175B parameters, Meta Llama 65B parameters) ones needs mammoth computing horsepower and large scale datasets to fine tune them on a specific task.\n",
        "- Fine tuning LLMs on specific tasks may lead them to “forget” previously learnt information, a phenomena known as *catastrophic forgetting*.\n",
        "\n",
        "So depending on the situation and goals of the adaptation of LLMs, it may be better to use adapters instead.\n",
        "\n",
        "Adapters are new modules added to a pre-trained model. In adapter-based learning, only the parameters in these new modules are trained while the original LLM is frozen, hence we learn a very small proportion of parameters. This means that the model has perfect memory of previous tasks and uses a small number of new parameters to learn the new task.\n",
        "\n",
        "Houlsby et.al. highlight benefits of adapter-based techniques:\n",
        "\n",
        "- Attains high performance.\n",
        "- Permits training on tasks sequentially, that is, it does not require simultaneous access to all datasets.\n",
        "- Adds only a small number of additional parameters per task (Q: why is this good?).\n",
        "- Model retains memory of previous tasks (learned during pre-training)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AoypRhE9lNy"
      },
      "outputs": [],
      "source": [
        "def encode_batch(batch):\n",
        "    try:\n",
        "        # Tokenize the input data\n",
        "        encoded_batch = tokenizer(batch[\"input\"], max_length=130, truncation=True, padding=\"max_length\")\n",
        "        return {\n",
        "            \"input_ids\": encoded_batch['input_ids'],\n",
        "            \"attention_mask\": encoded_batch['attention_mask'],\n",
        "            \"labels\": batch[\"label\"]\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing batch: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"for split in dataset.keys():\n",
        "    dataset[split] = dataset[split].map(\n",
        "        encode_batch,\n",
        "        batched=True,\n",
        "        remove_columns=[\"input\", \"output\",\"__index_level_0__\"],  # Adjust according to your dataset\n",
        "        load_from_cache_file=False\n",
        "    )\"\"\""
      ],
      "metadata": {
        "id": "Y6sSlqNjMj7W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b3cb434d-d3ee-4a62-e3db-adaa89b1b269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'for split in dataset.keys():\\n    dataset[split] = dataset[split].map(\\n        encode_batch,\\n        batched=True,\\n        remove_columns=[\"input\", \"output\",\"__index_level_0__\"],  # Adjust according to your dataset\\n        load_from_cache_file=False\\n    )'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    dataset['train'] = dataset['train'].map(\n",
        "        encode_batch,\n",
        "        batched=True,\n",
        "        remove_columns=[\"input\", \"output\",'__index_level_0__'],  # Adjust according to your dataset\n",
        "        load_from_cache_file=False\n",
        "    )\n",
        "\n",
        "    dataset['validation'] = dataset['validation'].map(\n",
        "        encode_batch,\n",
        "        batched=True,\n",
        "        remove_columns=[\"input\", \"output\"],  # Adjust according to your dataset\n",
        "        load_from_cache_file=False\n",
        "    )\n",
        "\n",
        "    dataset['test'] = dataset['test'].map(\n",
        "        encode_batch,\n",
        "        batched=True,\n",
        "        remove_columns=[\"input\", \"output\"],  # Adjust according to your dataset\n",
        "        load_from_cache_file=False\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "eaa65468c3ee4298b860fbaf984cb6d1",
            "295da6388a74422d93448666f95ac89d",
            "2fd6ce67797e409491a8655b35756bb9",
            "08b26fad036a455496a409b8fc1a714c",
            "247868c6fb0e49e281030722ff323c54",
            "70adc9a911cc42de87fb6fd1abcd738a",
            "ac2414d9a7de4d4c891e9d22831751ca",
            "28ffc1ec9dbc4936a33fc53e2a4e23a1",
            "8ef6abe308c6467db50ba964e00ec3c7",
            "15ddbf07fa384f4183481dec1ac28297",
            "c496a74b74f04ba8908d88d9bb2a00f5",
            "8080e3ac1f9b4769a513d97dfb8d0e6b",
            "ec1de94c2ad24b6da0ab82f4dc6173cf",
            "a532669b63bc4a0299ccc31ad0f77112",
            "4a5b837b4157417eb68c945500e6e770",
            "0d5cf14855934763b8f156acd7931473",
            "2872eef708ec4bbeb89f5abd29672d9a",
            "8fa19eef8ef5438eaa2222a187baf315",
            "bb7f7cbdc53640c2bf72e2055269f2d7",
            "895671f1518940d19558f14d00b9350e",
            "8f8624e134ed4816be008f28995baf67",
            "08ef934da68b4290b2095f38dfa21b99",
            "c97c91d1eba24ec5bb1bf742749112ed",
            "17f647db2d294704986c298fc119b492",
            "754726761de8491ba22aa404f2f20e92",
            "af3c9243fdf7493bb4d7b960fdf628c4",
            "f73b66ad25c2445e92f6c1fc492635a5",
            "5c6db585a9be4721911745b8ff6400c6",
            "5b6468a67c0d41f7acddac8a17498cf9",
            "54fe5231124d41bca3590d49e46e7e38",
            "6102c32b0e6d4af584a398312e19d297",
            "3dc8f9e89d51473d8060b821a8178468",
            "62b5bee40af542c296875f76f5e64278"
          ]
        },
        "id": "wWxniqqgtBQx",
        "outputId": "005d6ef4-8031-44d8-c05a-719441d6eada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3390 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eaa65468c3ee4298b860fbaf984cb6d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/281 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8080e3ac1f9b4769a513d97dfb8d0e6b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/281 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c97c91d1eba24ec5bb1bf742749112ed"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for split in dataset.keys():\n",
        "    print(f\"{split} columns: {dataset[split].column_names}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5lI-zvjM_El",
        "outputId": "4da0e7e6-255a-4c22-c53d-c443e6d45b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train columns: ['label', 'input_ids', 'attention_mask', 'labels']\n",
            "validation columns: ['label', 'input_ids', 'attention_mask', 'labels']\n",
            "test columns: ['label', 'input_ids', 'attention_mask', 'labels']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for split in dataset.keys():\n",
        "    if all(col in dataset[split].column_names for col in [\"input_ids\", \"attention_mask\", \"labels\"]):\n",
        "        dataset[split].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "    else:\n",
        "        print(f\"Required columns missing in {split} dataset. Cannot set format.\")"
      ],
      "metadata": {
        "id": "7P3Yp4WoNHju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdUkMk1L0zmH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fc935b3-3f66-47ca-e21a-e73008d675ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--bertweet-base/snapshots/118ab1d567653bec16bbb081eafb6f8942f72108/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"vinai/bertweet-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"comment\",\n",
            "    \"1\": \"support\",\n",
            "    \"2\": \"deny\",\n",
            "    \"3\": \"query\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 130,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64001\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--bertweet-base/snapshots/118ab1d567653bec16bbb081eafb6f8942f72108/pytorch_model.bin\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--bertweet-base/snapshots/118ab1d567653bec16bbb081eafb6f8942f72108/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"vinai/bertweet-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 130,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64001\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--vinai--bertweet-base/snapshots/118ab1d567653bec16bbb081eafb6f8942f72108/vocab.txt\n",
            "loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--vinai--bertweet-base/snapshots/118ab1d567653bec16bbb081eafb6f8942f72108/bpe.codes\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--bertweet-base/snapshots/118ab1d567653bec16bbb081eafb6f8942f72108/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"vinai/bertweet-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 130,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64001\n",
            "}\n",
            "\n",
            "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
            "Adding <mask> to the vocabulary\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Adding adapter 'sst'.\n"
          ]
        }
      ],
      "source": [
        "# @title We define the features of the model we want to use\n",
        "\n",
        "\n",
        "# Load the BERT configuration with custom settings\n",
        "config = AutoConfig.from_pretrained(\n",
        "    \"vinai/bertweet-base\",  # Specifies the pre-trained model, in this case, BERT base uncased (110M parameters)\n",
        "    num_labels=4,  # Number of labels for classification (2 in this case, representing binary sentiment classification)\n",
        "    id2label={0: \"comment\", 1: \"support\", 2: 'deny', 3: \"query\"}  # Mapping from label indices to human-readable labels\n",
        ")\n",
        "\n",
        "# Load the pre-trained BERT model for sequence classification with the above configuration\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"vinai/bertweet-base\",  # Specifies the pre-trained model to use\n",
        "    config=config  # The configuration object defining model parameters\n",
        ")\n",
        "\n",
        "# Load the tokenizer for the 'bert-base-uncased' model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
        "\n",
        "\n",
        "# Adding an adapter to the model\n",
        "model.add_adapter(\"sst\")  # Adds an adapter with the name 'sst'\n",
        "\n",
        "# This step configures the model to update only the adapter parameters during training.\n",
        "model.train_adapter([\"sst\"])  # Specifies that only the 'sst' adapter should be trained\n",
        "\n",
        "# This step tells the model to use the specified adapter(s) during inference and/or training.\n",
        "model.set_active_adapters([\"sst\"])  # Activates the 'sst' adapter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/Shareddrives/Computational Semantics/Assignmen 3/Graded part/Scripts/data\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoiNX0qQ4anJ",
        "outputId": "dc8a9add-7b16-4393-bc99-75ee9de52729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Shareddrives/Computational Semantics/Assignmen 3/Graded part/Scripts/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JJ-gqZ_4UCE",
        "outputId": "0b83c251-d7df-420b-d1aa-6881650c8a40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Shareddrives/Computational Semantics/Assignmen 3/Graded part/Scripts/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1g0pXJ90zql",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ec682c4-c56d-429c-d371-4da169071657"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "# @title We define some parameters concerning our training steps\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    learning_rate=1e-05, # The learning rate determines how big a step the model should take in the direction indicated by the gradient. A higher learning rate means taking bigger steps, and a lower learning rate means taking smaller steps.\n",
        "    num_train_epochs=80 #@param #  # Number of times the model will see the entire training data,\n",
        "                                  # start with a small number of epochs to try out the whole process. Later, increase it: more epochs should lead to better results\n",
        "    ,\n",
        "    per_device_train_batch_size=8, # Number of training examples processed at once per device\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_steps=100, # How often to print out logs during training\n",
        "    output_dir=\"./training_output_BERTtweet_clean_input_bert_80_epochs\", # Where to save the training results\n",
        "    overwrite_output_dir=True,\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "\n",
        "# the following part is needed to keep track of the \"loss\" during training (see below)\n",
        "class LossLoggingCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        self.losses = []\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "      print(f\"Logging at step {state.global_step}: {logs}\")\n",
        "      # Save the loss\n",
        "      if 'loss' in logs:\n",
        "          self.losses.append(logs['loss'])\n",
        "# Initialize the callback\n",
        "loss_logging_callback = LossLoggingCallback()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Zq9bqqIhJlQ"
      },
      "outputs": [],
      "source": [
        "# we define a function that we'll use to evaluate our model's performances\n",
        "def compute_accuracy(p: EvalPrediction):\n",
        "  preds = np.argmax(p.predictions, axis=1)\n",
        "  print((preds == p.label_ids).mean()) # This part calculates the accuracy of the model.\n",
        "                                       # It compares the predicted labels (preds) with the true labels (p.label_ids) element-wise,\n",
        "                                       # resulting in a boolean array where True represents a correct prediction and False represents an incorrect one.\n",
        "                                       # Taking the mean of this array gives the proportion of correct predictions, i.e., the accuracy.\n",
        "  return {\"eval_acc\": (preds == p.label_ids).mean()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-WIcYKTyDWJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c3b3a0c-4261-4e35-ca0d-df9fc8a781d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['label', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 3390\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['label', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 281\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['label', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 281\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and finally TRAIN our adapter!\n",
        "\n",
        "trainer = AdapterTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        "    compute_metrics=compute_accuracy,\n",
        "    callbacks=[loss_logging_callback]\n",
        ")\n",
        "#trainer.model.to('cuda')"
      ],
      "metadata": {
        "id": "vaqbGeHQpoTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdQNPYmQ2NWl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a00ea0c6-db4c-44ff-dfd1-d621570c3b77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 3390\n",
            "  Num Epochs = 80\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 33920\n",
            "  Number of trainable parameters = 1488196\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='33920' max='33920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [33920/33920 1:14:54, Epoch 80/80]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.186200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.041700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.010100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.977500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.027900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.969200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.008400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.983400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.964800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.933200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.961800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.941000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.969100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.913200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.986000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.905000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.883600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.915700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.866700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.919900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.924100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.903200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.933400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.890300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.873300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.897000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.892400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.861700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.884800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.839500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>0.898500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>0.829700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>0.865700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>0.796100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.836200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>0.837200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>0.842600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>0.826300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>0.861200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.792700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4100</td>\n",
              "      <td>0.875400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>0.792000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4300</td>\n",
              "      <td>0.757900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>0.855800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.848900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4600</td>\n",
              "      <td>0.826800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4700</td>\n",
              "      <td>0.819100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>0.823100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4900</td>\n",
              "      <td>0.780400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.800700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5100</td>\n",
              "      <td>0.837800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5200</td>\n",
              "      <td>0.783200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5300</td>\n",
              "      <td>0.831800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5400</td>\n",
              "      <td>0.790800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.825200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5600</td>\n",
              "      <td>0.786000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5700</td>\n",
              "      <td>0.829600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5800</td>\n",
              "      <td>0.794500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5900</td>\n",
              "      <td>0.747900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.812000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6100</td>\n",
              "      <td>0.795200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6200</td>\n",
              "      <td>0.822800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6300</td>\n",
              "      <td>0.785600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6400</td>\n",
              "      <td>0.751700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.764300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6600</td>\n",
              "      <td>0.805800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6700</td>\n",
              "      <td>0.813200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6800</td>\n",
              "      <td>0.775400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6900</td>\n",
              "      <td>0.726600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.846700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7100</td>\n",
              "      <td>0.794500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7200</td>\n",
              "      <td>0.765300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7300</td>\n",
              "      <td>0.811300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7400</td>\n",
              "      <td>0.774800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.773300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7600</td>\n",
              "      <td>0.757000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7700</td>\n",
              "      <td>0.816800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7800</td>\n",
              "      <td>0.782700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7900</td>\n",
              "      <td>0.740800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.763200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8100</td>\n",
              "      <td>0.758300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8200</td>\n",
              "      <td>0.763500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8300</td>\n",
              "      <td>0.742800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8400</td>\n",
              "      <td>0.771500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.780300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8600</td>\n",
              "      <td>0.727700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8700</td>\n",
              "      <td>0.779600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8800</td>\n",
              "      <td>0.768400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8900</td>\n",
              "      <td>0.759700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.757800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9100</td>\n",
              "      <td>0.737200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9200</td>\n",
              "      <td>0.785000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9300</td>\n",
              "      <td>0.780900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9400</td>\n",
              "      <td>0.707800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.802600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9600</td>\n",
              "      <td>0.750100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9700</td>\n",
              "      <td>0.729400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9800</td>\n",
              "      <td>0.759000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9900</td>\n",
              "      <td>0.722000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.749400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10100</td>\n",
              "      <td>0.744200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10200</td>\n",
              "      <td>0.753200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10300</td>\n",
              "      <td>0.797300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10400</td>\n",
              "      <td>0.726400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.695500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10600</td>\n",
              "      <td>0.762200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10700</td>\n",
              "      <td>0.765200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10800</td>\n",
              "      <td>0.687500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10900</td>\n",
              "      <td>0.727500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.757800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11100</td>\n",
              "      <td>0.836500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11200</td>\n",
              "      <td>0.749400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11300</td>\n",
              "      <td>0.713300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11400</td>\n",
              "      <td>0.709900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.732900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11600</td>\n",
              "      <td>0.705400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11700</td>\n",
              "      <td>0.711900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11800</td>\n",
              "      <td>0.772000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11900</td>\n",
              "      <td>0.755000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.734600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12100</td>\n",
              "      <td>0.691200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12200</td>\n",
              "      <td>0.735700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12300</td>\n",
              "      <td>0.743000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12400</td>\n",
              "      <td>0.731000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>0.741600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12600</td>\n",
              "      <td>0.722500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12700</td>\n",
              "      <td>0.715000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12800</td>\n",
              "      <td>0.747900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12900</td>\n",
              "      <td>0.739900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.696900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13100</td>\n",
              "      <td>0.722600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13200</td>\n",
              "      <td>0.702200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13300</td>\n",
              "      <td>0.694800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13400</td>\n",
              "      <td>0.706300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>0.730900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13600</td>\n",
              "      <td>0.705300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13700</td>\n",
              "      <td>0.695100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13800</td>\n",
              "      <td>0.719800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13900</td>\n",
              "      <td>0.737400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.708800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14100</td>\n",
              "      <td>0.754400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14200</td>\n",
              "      <td>0.660700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14300</td>\n",
              "      <td>0.727500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14400</td>\n",
              "      <td>0.720100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>0.688400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14600</td>\n",
              "      <td>0.691100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14700</td>\n",
              "      <td>0.716900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14800</td>\n",
              "      <td>0.746900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14900</td>\n",
              "      <td>0.726700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.676400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15100</td>\n",
              "      <td>0.728200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15200</td>\n",
              "      <td>0.705300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15300</td>\n",
              "      <td>0.682400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15400</td>\n",
              "      <td>0.730500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>0.737100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15600</td>\n",
              "      <td>0.660300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15700</td>\n",
              "      <td>0.668300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15800</td>\n",
              "      <td>0.688400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15900</td>\n",
              "      <td>0.715500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.697700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16100</td>\n",
              "      <td>0.684400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16200</td>\n",
              "      <td>0.721700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16300</td>\n",
              "      <td>0.662700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16400</td>\n",
              "      <td>0.724600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>0.707200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16600</td>\n",
              "      <td>0.708200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16700</td>\n",
              "      <td>0.718700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16800</td>\n",
              "      <td>0.725600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16900</td>\n",
              "      <td>0.678600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>0.662400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17100</td>\n",
              "      <td>0.678300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17200</td>\n",
              "      <td>0.769900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17300</td>\n",
              "      <td>0.647400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17400</td>\n",
              "      <td>0.655400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>0.679000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17600</td>\n",
              "      <td>0.667600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17700</td>\n",
              "      <td>0.691200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17800</td>\n",
              "      <td>0.737700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17900</td>\n",
              "      <td>0.713600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.666700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18100</td>\n",
              "      <td>0.674100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18200</td>\n",
              "      <td>0.713700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18300</td>\n",
              "      <td>0.679900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18400</td>\n",
              "      <td>0.655900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>0.685600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18600</td>\n",
              "      <td>0.718300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18700</td>\n",
              "      <td>0.659100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18800</td>\n",
              "      <td>0.697000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18900</td>\n",
              "      <td>0.677100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>0.676900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19100</td>\n",
              "      <td>0.688300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19200</td>\n",
              "      <td>0.701200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19300</td>\n",
              "      <td>0.661300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19400</td>\n",
              "      <td>0.695200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>0.659900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19600</td>\n",
              "      <td>0.747600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19700</td>\n",
              "      <td>0.662000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19800</td>\n",
              "      <td>0.652100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19900</td>\n",
              "      <td>0.684100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.656500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20100</td>\n",
              "      <td>0.727700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20200</td>\n",
              "      <td>0.659400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20300</td>\n",
              "      <td>0.682100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20400</td>\n",
              "      <td>0.680500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20500</td>\n",
              "      <td>0.632900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20600</td>\n",
              "      <td>0.656700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20700</td>\n",
              "      <td>0.690000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20800</td>\n",
              "      <td>0.670400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20900</td>\n",
              "      <td>0.652000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>0.666200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21100</td>\n",
              "      <td>0.680600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21200</td>\n",
              "      <td>0.690300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21300</td>\n",
              "      <td>0.677300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21400</td>\n",
              "      <td>0.635100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21500</td>\n",
              "      <td>0.703500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21600</td>\n",
              "      <td>0.684900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21700</td>\n",
              "      <td>0.626500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21800</td>\n",
              "      <td>0.686300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21900</td>\n",
              "      <td>0.689600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>0.665700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22100</td>\n",
              "      <td>0.637600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22200</td>\n",
              "      <td>0.670900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22300</td>\n",
              "      <td>0.656000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22400</td>\n",
              "      <td>0.698900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22500</td>\n",
              "      <td>0.639600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22600</td>\n",
              "      <td>0.666600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22700</td>\n",
              "      <td>0.638400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22800</td>\n",
              "      <td>0.633400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22900</td>\n",
              "      <td>0.723900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>0.645000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23100</td>\n",
              "      <td>0.661500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23200</td>\n",
              "      <td>0.653900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23300</td>\n",
              "      <td>0.659200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23400</td>\n",
              "      <td>0.710500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23500</td>\n",
              "      <td>0.619000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23600</td>\n",
              "      <td>0.689100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23700</td>\n",
              "      <td>0.643000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23800</td>\n",
              "      <td>0.638100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23900</td>\n",
              "      <td>0.679100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>0.682700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24100</td>\n",
              "      <td>0.632900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24200</td>\n",
              "      <td>0.641900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24300</td>\n",
              "      <td>0.641000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24400</td>\n",
              "      <td>0.677500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24500</td>\n",
              "      <td>0.688400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24600</td>\n",
              "      <td>0.632100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24700</td>\n",
              "      <td>0.691300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24800</td>\n",
              "      <td>0.627600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24900</td>\n",
              "      <td>0.656700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>0.648800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25100</td>\n",
              "      <td>0.645000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25200</td>\n",
              "      <td>0.680100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25300</td>\n",
              "      <td>0.669300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25400</td>\n",
              "      <td>0.621000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25500</td>\n",
              "      <td>0.627400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25600</td>\n",
              "      <td>0.702300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25700</td>\n",
              "      <td>0.673000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25800</td>\n",
              "      <td>0.643600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25900</td>\n",
              "      <td>0.628800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26000</td>\n",
              "      <td>0.674500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26100</td>\n",
              "      <td>0.615700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26200</td>\n",
              "      <td>0.650900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26300</td>\n",
              "      <td>0.682500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26400</td>\n",
              "      <td>0.636900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26500</td>\n",
              "      <td>0.704000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26600</td>\n",
              "      <td>0.612700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26700</td>\n",
              "      <td>0.661600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26800</td>\n",
              "      <td>0.638900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26900</td>\n",
              "      <td>0.636900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27000</td>\n",
              "      <td>0.686900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27100</td>\n",
              "      <td>0.636300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27200</td>\n",
              "      <td>0.678000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27300</td>\n",
              "      <td>0.642400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27400</td>\n",
              "      <td>0.616900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27500</td>\n",
              "      <td>0.681300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27600</td>\n",
              "      <td>0.629100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27700</td>\n",
              "      <td>0.650800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27800</td>\n",
              "      <td>0.641400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27900</td>\n",
              "      <td>0.673800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28000</td>\n",
              "      <td>0.635800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28100</td>\n",
              "      <td>0.694400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28200</td>\n",
              "      <td>0.645900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28300</td>\n",
              "      <td>0.631300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28400</td>\n",
              "      <td>0.645900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28500</td>\n",
              "      <td>0.586800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28600</td>\n",
              "      <td>0.613700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28700</td>\n",
              "      <td>0.705500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28800</td>\n",
              "      <td>0.675000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28900</td>\n",
              "      <td>0.685300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29000</td>\n",
              "      <td>0.718000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29100</td>\n",
              "      <td>0.618300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29200</td>\n",
              "      <td>0.597000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29300</td>\n",
              "      <td>0.615300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29400</td>\n",
              "      <td>0.583900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29500</td>\n",
              "      <td>0.594400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29600</td>\n",
              "      <td>0.725000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29700</td>\n",
              "      <td>0.672400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29800</td>\n",
              "      <td>0.645600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29900</td>\n",
              "      <td>0.604900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30000</td>\n",
              "      <td>0.710900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30100</td>\n",
              "      <td>0.643200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30200</td>\n",
              "      <td>0.631300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30300</td>\n",
              "      <td>0.634100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30400</td>\n",
              "      <td>0.664600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30500</td>\n",
              "      <td>0.620100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30600</td>\n",
              "      <td>0.680800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30700</td>\n",
              "      <td>0.665500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30800</td>\n",
              "      <td>0.583600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30900</td>\n",
              "      <td>0.643200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31000</td>\n",
              "      <td>0.647600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31100</td>\n",
              "      <td>0.656600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31200</td>\n",
              "      <td>0.657300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31300</td>\n",
              "      <td>0.610400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31400</td>\n",
              "      <td>0.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31500</td>\n",
              "      <td>0.617000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31600</td>\n",
              "      <td>0.651900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31700</td>\n",
              "      <td>0.655600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31800</td>\n",
              "      <td>0.651900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31900</td>\n",
              "      <td>0.628300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32000</td>\n",
              "      <td>0.593800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32100</td>\n",
              "      <td>0.676900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32200</td>\n",
              "      <td>0.648700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32300</td>\n",
              "      <td>0.651700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32400</td>\n",
              "      <td>0.642200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32500</td>\n",
              "      <td>0.589800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32600</td>\n",
              "      <td>0.662000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32700</td>\n",
              "      <td>0.636100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32800</td>\n",
              "      <td>0.613000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32900</td>\n",
              "      <td>0.629500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33000</td>\n",
              "      <td>0.627500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33100</td>\n",
              "      <td>0.696800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33200</td>\n",
              "      <td>0.623700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33300</td>\n",
              "      <td>0.624000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33400</td>\n",
              "      <td>0.633200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33500</td>\n",
              "      <td>0.657700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33600</td>\n",
              "      <td>0.624400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33700</td>\n",
              "      <td>0.664600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33800</td>\n",
              "      <td>0.632400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33900</td>\n",
              "      <td>0.654200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 100: {'loss': 1.1862, 'learning_rate': 9.970518867924528e-06, 'epoch': 0.24}\n",
            "Logging at step 200: {'loss': 1.0417, 'learning_rate': 9.941037735849058e-06, 'epoch': 0.47}\n",
            "Logging at step 300: {'loss': 1.0101, 'learning_rate': 9.911556603773585e-06, 'epoch': 0.71}\n",
            "Logging at step 400: {'loss': 0.9775, 'learning_rate': 9.882075471698113e-06, 'epoch': 0.94}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 500: {'loss': 1.0279, 'learning_rate': 9.852594339622642e-06, 'epoch': 1.18}\n",
            "Logging at step 600: {'loss': 0.9692, 'learning_rate': 9.82311320754717e-06, 'epoch': 1.42}\n",
            "Logging at step 700: {'loss': 1.0084, 'learning_rate': 9.7936320754717e-06, 'epoch': 1.65}\n",
            "Logging at step 800: {'loss': 0.9834, 'learning_rate': 9.764150943396227e-06, 'epoch': 1.89}\n",
            "Logging at step 900: {'loss': 0.9648, 'learning_rate': 9.734669811320756e-06, 'epoch': 2.12}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-1000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-1000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-1000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-1000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-1000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 1000: {'loss': 0.9332, 'learning_rate': 9.705188679245284e-06, 'epoch': 2.36}\n",
            "Logging at step 1100: {'loss': 0.9618, 'learning_rate': 9.675707547169813e-06, 'epoch': 2.59}\n",
            "Logging at step 1200: {'loss': 0.941, 'learning_rate': 9.64622641509434e-06, 'epoch': 2.83}\n",
            "Logging at step 1300: {'loss': 0.9691, 'learning_rate': 9.616745283018868e-06, 'epoch': 3.07}\n",
            "Logging at step 1400: {'loss': 0.9132, 'learning_rate': 9.587264150943398e-06, 'epoch': 3.3}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-1500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-1500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-1500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-1500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-1500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 1500: {'loss': 0.986, 'learning_rate': 9.557783018867925e-06, 'epoch': 3.54}\n",
            "Logging at step 1600: {'loss': 0.905, 'learning_rate': 9.528301886792455e-06, 'epoch': 3.77}\n",
            "Logging at step 1700: {'loss': 0.8836, 'learning_rate': 9.498820754716982e-06, 'epoch': 4.01}\n",
            "Logging at step 1800: {'loss': 0.9157, 'learning_rate': 9.46933962264151e-06, 'epoch': 4.25}\n",
            "Logging at step 1900: {'loss': 0.8667, 'learning_rate': 9.439858490566039e-06, 'epoch': 4.48}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-2000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-2000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-2000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-2000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-2000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 2000: {'loss': 0.9199, 'learning_rate': 9.410377358490567e-06, 'epoch': 4.72}\n",
            "Logging at step 2100: {'loss': 0.9241, 'learning_rate': 9.380896226415094e-06, 'epoch': 4.95}\n",
            "Logging at step 2200: {'loss': 0.9032, 'learning_rate': 9.351415094339624e-06, 'epoch': 5.19}\n",
            "Logging at step 2300: {'loss': 0.9334, 'learning_rate': 9.321933962264151e-06, 'epoch': 5.42}\n",
            "Logging at step 2400: {'loss': 0.8903, 'learning_rate': 9.292452830188679e-06, 'epoch': 5.66}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-2500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-2500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-2500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-2500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-2500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 2500: {'loss': 0.8733, 'learning_rate': 9.262971698113208e-06, 'epoch': 5.9}\n",
            "Logging at step 2600: {'loss': 0.897, 'learning_rate': 9.233490566037736e-06, 'epoch': 6.13}\n",
            "Logging at step 2700: {'loss': 0.8924, 'learning_rate': 9.204009433962265e-06, 'epoch': 6.37}\n",
            "Logging at step 2800: {'loss': 0.8617, 'learning_rate': 9.174528301886794e-06, 'epoch': 6.6}\n",
            "Logging at step 2900: {'loss': 0.8848, 'learning_rate': 9.145047169811322e-06, 'epoch': 6.84}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-3000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-3000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-3000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-3000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-3000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 3000: {'loss': 0.8395, 'learning_rate': 9.11556603773585e-06, 'epoch': 7.08}\n",
            "Logging at step 3100: {'loss': 0.8985, 'learning_rate': 9.086084905660379e-06, 'epoch': 7.31}\n",
            "Logging at step 3200: {'loss': 0.8297, 'learning_rate': 9.056603773584907e-06, 'epoch': 7.55}\n",
            "Logging at step 3300: {'loss': 0.8657, 'learning_rate': 9.027122641509434e-06, 'epoch': 7.78}\n",
            "Logging at step 3400: {'loss': 0.7961, 'learning_rate': 8.997641509433964e-06, 'epoch': 8.02}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-3500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-3500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-3500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-3500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-3500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 3500: {'loss': 0.8362, 'learning_rate': 8.968160377358491e-06, 'epoch': 8.25}\n",
            "Logging at step 3600: {'loss': 0.8372, 'learning_rate': 8.938679245283019e-06, 'epoch': 8.49}\n",
            "Logging at step 3700: {'loss': 0.8426, 'learning_rate': 8.909198113207548e-06, 'epoch': 8.73}\n",
            "Logging at step 3800: {'loss': 0.8263, 'learning_rate': 8.879716981132076e-06, 'epoch': 8.96}\n",
            "Logging at step 3900: {'loss': 0.8612, 'learning_rate': 8.850235849056605e-06, 'epoch': 9.2}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-4000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-4000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-4000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-4000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-4000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 4000: {'loss': 0.7927, 'learning_rate': 8.820754716981133e-06, 'epoch': 9.43}\n",
            "Logging at step 4100: {'loss': 0.8754, 'learning_rate': 8.79127358490566e-06, 'epoch': 9.67}\n",
            "Logging at step 4200: {'loss': 0.792, 'learning_rate': 8.76179245283019e-06, 'epoch': 9.91}\n",
            "Logging at step 4300: {'loss': 0.7579, 'learning_rate': 8.732311320754717e-06, 'epoch': 10.14}\n",
            "Logging at step 4400: {'loss': 0.8558, 'learning_rate': 8.702830188679245e-06, 'epoch': 10.38}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-4500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-4500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-4500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-4500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-4500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 4500: {'loss': 0.8489, 'learning_rate': 8.673349056603774e-06, 'epoch': 10.61}\n",
            "Logging at step 4600: {'loss': 0.8268, 'learning_rate': 8.643867924528303e-06, 'epoch': 10.85}\n",
            "Logging at step 4700: {'loss': 0.8191, 'learning_rate': 8.614386792452831e-06, 'epoch': 11.08}\n",
            "Logging at step 4800: {'loss': 0.8231, 'learning_rate': 8.58490566037736e-06, 'epoch': 11.32}\n",
            "Logging at step 4900: {'loss': 0.7804, 'learning_rate': 8.555424528301888e-06, 'epoch': 11.56}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-5000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-5000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-5000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-5000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-5000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 5000: {'loss': 0.8007, 'learning_rate': 8.525943396226416e-06, 'epoch': 11.79}\n",
            "Logging at step 5100: {'loss': 0.8378, 'learning_rate': 8.496462264150945e-06, 'epoch': 12.03}\n",
            "Logging at step 5200: {'loss': 0.7832, 'learning_rate': 8.466981132075472e-06, 'epoch': 12.26}\n",
            "Logging at step 5300: {'loss': 0.8318, 'learning_rate': 8.4375e-06, 'epoch': 12.5}\n",
            "Logging at step 5400: {'loss': 0.7908, 'learning_rate': 8.40801886792453e-06, 'epoch': 12.74}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-5500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-5500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-5500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-5500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-5500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 5500: {'loss': 0.8252, 'learning_rate': 8.378537735849057e-06, 'epoch': 12.97}\n",
            "Logging at step 5600: {'loss': 0.786, 'learning_rate': 8.349056603773585e-06, 'epoch': 13.21}\n",
            "Logging at step 5700: {'loss': 0.8296, 'learning_rate': 8.319575471698114e-06, 'epoch': 13.44}\n",
            "Logging at step 5800: {'loss': 0.7945, 'learning_rate': 8.290094339622642e-06, 'epoch': 13.68}\n",
            "Logging at step 5900: {'loss': 0.7479, 'learning_rate': 8.260613207547171e-06, 'epoch': 13.92}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-6000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-6000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-6000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-6000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-6000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 6000: {'loss': 0.812, 'learning_rate': 8.231132075471699e-06, 'epoch': 14.15}\n",
            "Logging at step 6100: {'loss': 0.7952, 'learning_rate': 8.201650943396226e-06, 'epoch': 14.39}\n",
            "Logging at step 6200: {'loss': 0.8228, 'learning_rate': 8.172169811320755e-06, 'epoch': 14.62}\n",
            "Logging at step 6300: {'loss': 0.7856, 'learning_rate': 8.142688679245285e-06, 'epoch': 14.86}\n",
            "Logging at step 6400: {'loss': 0.7517, 'learning_rate': 8.113207547169812e-06, 'epoch': 15.09}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-6500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-6500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-6500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-6500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-6500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 6500: {'loss': 0.7643, 'learning_rate': 8.08372641509434e-06, 'epoch': 15.33}\n",
            "Logging at step 6600: {'loss': 0.8058, 'learning_rate': 8.05424528301887e-06, 'epoch': 15.57}\n",
            "Logging at step 6700: {'loss': 0.8132, 'learning_rate': 8.024764150943397e-06, 'epoch': 15.8}\n",
            "Logging at step 6800: {'loss': 0.7754, 'learning_rate': 7.995283018867925e-06, 'epoch': 16.04}\n",
            "Logging at step 6900: {'loss': 0.7266, 'learning_rate': 7.965801886792454e-06, 'epoch': 16.27}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-7000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-7000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-7000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-7000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-7000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 7000: {'loss': 0.8467, 'learning_rate': 7.936320754716981e-06, 'epoch': 16.51}\n",
            "Logging at step 7100: {'loss': 0.7945, 'learning_rate': 7.90683962264151e-06, 'epoch': 16.75}\n",
            "Logging at step 7200: {'loss': 0.7653, 'learning_rate': 7.877358490566038e-06, 'epoch': 16.98}\n",
            "Logging at step 7300: {'loss': 0.8113, 'learning_rate': 7.847877358490566e-06, 'epoch': 17.22}\n",
            "Logging at step 7400: {'loss': 0.7748, 'learning_rate': 7.818396226415095e-06, 'epoch': 17.45}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-7500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-7500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-7500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-7500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-7500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 7500: {'loss': 0.7733, 'learning_rate': 7.788915094339623e-06, 'epoch': 17.69}\n",
            "Logging at step 7600: {'loss': 0.757, 'learning_rate': 7.75943396226415e-06, 'epoch': 17.92}\n",
            "Logging at step 7700: {'loss': 0.8168, 'learning_rate': 7.72995283018868e-06, 'epoch': 18.16}\n",
            "Logging at step 7800: {'loss': 0.7827, 'learning_rate': 7.700471698113207e-06, 'epoch': 18.4}\n",
            "Logging at step 7900: {'loss': 0.7408, 'learning_rate': 7.670990566037737e-06, 'epoch': 18.63}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-8000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-8000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-8000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-8000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-8000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 8000: {'loss': 0.7632, 'learning_rate': 7.641509433962266e-06, 'epoch': 18.87}\n",
            "Logging at step 8100: {'loss': 0.7583, 'learning_rate': 7.612028301886794e-06, 'epoch': 19.1}\n",
            "Logging at step 8200: {'loss': 0.7635, 'learning_rate': 7.582547169811322e-06, 'epoch': 19.34}\n",
            "Logging at step 8300: {'loss': 0.7428, 'learning_rate': 7.55306603773585e-06, 'epoch': 19.58}\n",
            "Logging at step 8400: {'loss': 0.7715, 'learning_rate': 7.523584905660378e-06, 'epoch': 19.81}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-8500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-8500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-8500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-8500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-8500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 8500: {'loss': 0.7803, 'learning_rate': 7.494103773584907e-06, 'epoch': 20.05}\n",
            "Logging at step 8600: {'loss': 0.7277, 'learning_rate': 7.464622641509434e-06, 'epoch': 20.28}\n",
            "Logging at step 8700: {'loss': 0.7796, 'learning_rate': 7.435141509433963e-06, 'epoch': 20.52}\n",
            "Logging at step 8800: {'loss': 0.7684, 'learning_rate': 7.405660377358491e-06, 'epoch': 20.75}\n",
            "Logging at step 8900: {'loss': 0.7597, 'learning_rate': 7.37617924528302e-06, 'epoch': 20.99}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-9000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-9000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-9000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-9000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-9000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 9000: {'loss': 0.7578, 'learning_rate': 7.346698113207547e-06, 'epoch': 21.23}\n",
            "Logging at step 9100: {'loss': 0.7372, 'learning_rate': 7.317216981132076e-06, 'epoch': 21.46}\n",
            "Logging at step 9200: {'loss': 0.785, 'learning_rate': 7.287735849056604e-06, 'epoch': 21.7}\n",
            "Logging at step 9300: {'loss': 0.7809, 'learning_rate': 7.258254716981133e-06, 'epoch': 21.93}\n",
            "Logging at step 9400: {'loss': 0.7078, 'learning_rate': 7.22877358490566e-06, 'epoch': 22.17}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-9500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-9500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-9500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-9500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-9500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 9500: {'loss': 0.8026, 'learning_rate': 7.199292452830189e-06, 'epoch': 22.41}\n",
            "Logging at step 9600: {'loss': 0.7501, 'learning_rate': 7.169811320754717e-06, 'epoch': 22.64}\n",
            "Logging at step 9700: {'loss': 0.7294, 'learning_rate': 7.140330188679245e-06, 'epoch': 22.88}\n",
            "Logging at step 9800: {'loss': 0.759, 'learning_rate': 7.110849056603775e-06, 'epoch': 23.11}\n",
            "Logging at step 9900: {'loss': 0.722, 'learning_rate': 7.081367924528303e-06, 'epoch': 23.35}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-10000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-10000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-10000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-10000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-10000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 10000: {'loss': 0.7494, 'learning_rate': 7.051886792452831e-06, 'epoch': 23.58}\n",
            "Logging at step 10100: {'loss': 0.7442, 'learning_rate': 7.02240566037736e-06, 'epoch': 23.82}\n",
            "Logging at step 10200: {'loss': 0.7532, 'learning_rate': 6.992924528301887e-06, 'epoch': 24.06}\n",
            "Logging at step 10300: {'loss': 0.7973, 'learning_rate': 6.963443396226416e-06, 'epoch': 24.29}\n",
            "Logging at step 10400: {'loss': 0.7264, 'learning_rate': 6.933962264150944e-06, 'epoch': 24.53}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-10500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-10500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-10500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-10500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-10500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 10500: {'loss': 0.6955, 'learning_rate': 6.904481132075473e-06, 'epoch': 24.76}\n",
            "Logging at step 10600: {'loss': 0.7622, 'learning_rate': 6.875e-06, 'epoch': 25.0}\n",
            "Logging at step 10700: {'loss': 0.7652, 'learning_rate': 6.845518867924529e-06, 'epoch': 25.24}\n",
            "Logging at step 10800: {'loss': 0.6875, 'learning_rate': 6.816037735849057e-06, 'epoch': 25.47}\n",
            "Logging at step 10900: {'loss': 0.7275, 'learning_rate': 6.786556603773586e-06, 'epoch': 25.71}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-11000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-11000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-11000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-11000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-11000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 11000: {'loss': 0.7578, 'learning_rate': 6.757075471698113e-06, 'epoch': 25.94}\n",
            "Logging at step 11100: {'loss': 0.8365, 'learning_rate': 6.727594339622642e-06, 'epoch': 26.18}\n",
            "Logging at step 11200: {'loss': 0.7494, 'learning_rate': 6.69811320754717e-06, 'epoch': 26.42}\n",
            "Logging at step 11300: {'loss': 0.7133, 'learning_rate': 6.668632075471698e-06, 'epoch': 26.65}\n",
            "Logging at step 11400: {'loss': 0.7099, 'learning_rate': 6.639150943396226e-06, 'epoch': 26.89}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-11500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-11500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-11500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-11500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-11500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 11500: {'loss': 0.7329, 'learning_rate': 6.6096698113207556e-06, 'epoch': 27.12}\n",
            "Logging at step 11600: {'loss': 0.7054, 'learning_rate': 6.580188679245284e-06, 'epoch': 27.36}\n",
            "Logging at step 11700: {'loss': 0.7119, 'learning_rate': 6.5507075471698125e-06, 'epoch': 27.59}\n",
            "Logging at step 11800: {'loss': 0.772, 'learning_rate': 6.52122641509434e-06, 'epoch': 27.83}\n",
            "Logging at step 11900: {'loss': 0.755, 'learning_rate': 6.491745283018869e-06, 'epoch': 28.07}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-12000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-12000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-12000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-12000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-12000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 12000: {'loss': 0.7346, 'learning_rate': 6.462264150943397e-06, 'epoch': 28.3}\n",
            "Logging at step 12100: {'loss': 0.6912, 'learning_rate': 6.4327830188679255e-06, 'epoch': 28.54}\n",
            "Logging at step 12200: {'loss': 0.7357, 'learning_rate': 6.403301886792453e-06, 'epoch': 28.77}\n",
            "Logging at step 12300: {'loss': 0.743, 'learning_rate': 6.373820754716982e-06, 'epoch': 29.01}\n",
            "Logging at step 12400: {'loss': 0.731, 'learning_rate': 6.34433962264151e-06, 'epoch': 29.25}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-12500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-12500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-12500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-12500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-12500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 12500: {'loss': 0.7416, 'learning_rate': 6.3148584905660385e-06, 'epoch': 29.48}\n",
            "Logging at step 12600: {'loss': 0.7225, 'learning_rate': 6.285377358490566e-06, 'epoch': 29.72}\n",
            "Logging at step 12700: {'loss': 0.715, 'learning_rate': 6.255896226415095e-06, 'epoch': 29.95}\n",
            "Logging at step 12800: {'loss': 0.7479, 'learning_rate': 6.226415094339623e-06, 'epoch': 30.19}\n",
            "Logging at step 12900: {'loss': 0.7399, 'learning_rate': 6.1969339622641515e-06, 'epoch': 30.42}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-13000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-13000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-13000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-13000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-13000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 13000: {'loss': 0.6969, 'learning_rate': 6.167452830188679e-06, 'epoch': 30.66}\n",
            "Logging at step 13100: {'loss': 0.7226, 'learning_rate': 6.137971698113208e-06, 'epoch': 30.9}\n",
            "Logging at step 13200: {'loss': 0.7022, 'learning_rate': 6.108490566037736e-06, 'epoch': 31.13}\n",
            "Logging at step 13300: {'loss': 0.6948, 'learning_rate': 6.079009433962265e-06, 'epoch': 31.37}\n",
            "Logging at step 13400: {'loss': 0.7063, 'learning_rate': 6.049528301886793e-06, 'epoch': 31.6}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-13500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-13500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-13500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-13500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-13500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 13500: {'loss': 0.7309, 'learning_rate': 6.0200471698113215e-06, 'epoch': 31.84}\n",
            "Logging at step 13600: {'loss': 0.7053, 'learning_rate': 5.99056603773585e-06, 'epoch': 32.08}\n",
            "Logging at step 13700: {'loss': 0.6951, 'learning_rate': 5.961084905660378e-06, 'epoch': 32.31}\n",
            "Logging at step 13800: {'loss': 0.7198, 'learning_rate': 5.931603773584906e-06, 'epoch': 32.55}\n",
            "Logging at step 13900: {'loss': 0.7374, 'learning_rate': 5.9021226415094345e-06, 'epoch': 32.78}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-14000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-14000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-14000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-14000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-14000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 14000: {'loss': 0.7088, 'learning_rate': 5.872641509433963e-06, 'epoch': 33.02}\n",
            "Logging at step 14100: {'loss': 0.7544, 'learning_rate': 5.843160377358491e-06, 'epoch': 33.25}\n",
            "Logging at step 14200: {'loss': 0.6607, 'learning_rate': 5.813679245283019e-06, 'epoch': 33.49}\n",
            "Logging at step 14300: {'loss': 0.7275, 'learning_rate': 5.7841981132075475e-06, 'epoch': 33.73}\n",
            "Logging at step 14400: {'loss': 0.7201, 'learning_rate': 5.754716981132076e-06, 'epoch': 33.96}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-14500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-14500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-14500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-14500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-14500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 14500: {'loss': 0.6884, 'learning_rate': 5.725235849056604e-06, 'epoch': 34.2}\n",
            "Logging at step 14600: {'loss': 0.6911, 'learning_rate': 5.695754716981132e-06, 'epoch': 34.43}\n",
            "Logging at step 14700: {'loss': 0.7169, 'learning_rate': 5.6662735849056605e-06, 'epoch': 34.67}\n",
            "Logging at step 14800: {'loss': 0.7469, 'learning_rate': 5.636792452830189e-06, 'epoch': 34.91}\n",
            "Logging at step 14900: {'loss': 0.7267, 'learning_rate': 5.607311320754717e-06, 'epoch': 35.14}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-15000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-15000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-15000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-15000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-15000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 15000: {'loss': 0.6764, 'learning_rate': 5.577830188679245e-06, 'epoch': 35.38}\n",
            "Logging at step 15100: {'loss': 0.7282, 'learning_rate': 5.548349056603774e-06, 'epoch': 35.61}\n",
            "Logging at step 15200: {'loss': 0.7053, 'learning_rate': 5.518867924528303e-06, 'epoch': 35.85}\n",
            "Logging at step 15300: {'loss': 0.6824, 'learning_rate': 5.489386792452831e-06, 'epoch': 36.08}\n",
            "Logging at step 15400: {'loss': 0.7305, 'learning_rate': 5.459905660377359e-06, 'epoch': 36.32}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-15500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-15500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-15500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-15500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-15500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 15500: {'loss': 0.7371, 'learning_rate': 5.430424528301887e-06, 'epoch': 36.56}\n",
            "Logging at step 15600: {'loss': 0.6603, 'learning_rate': 5.400943396226416e-06, 'epoch': 36.79}\n",
            "Logging at step 15700: {'loss': 0.6683, 'learning_rate': 5.371462264150944e-06, 'epoch': 37.03}\n",
            "Logging at step 15800: {'loss': 0.6884, 'learning_rate': 5.341981132075472e-06, 'epoch': 37.26}\n",
            "Logging at step 15900: {'loss': 0.7155, 'learning_rate': 5.3125e-06, 'epoch': 37.5}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-16000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-16000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-16000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-16000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-16000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 16000: {'loss': 0.6977, 'learning_rate': 5.283018867924529e-06, 'epoch': 37.74}\n",
            "Logging at step 16100: {'loss': 0.6844, 'learning_rate': 5.253537735849057e-06, 'epoch': 37.97}\n",
            "Logging at step 16200: {'loss': 0.7217, 'learning_rate': 5.224056603773585e-06, 'epoch': 38.21}\n",
            "Logging at step 16300: {'loss': 0.6627, 'learning_rate': 5.194575471698113e-06, 'epoch': 38.44}\n",
            "Logging at step 16400: {'loss': 0.7246, 'learning_rate': 5.165094339622642e-06, 'epoch': 38.68}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-16500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-16500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-16500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-16500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-16500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 16500: {'loss': 0.7072, 'learning_rate': 5.1356132075471695e-06, 'epoch': 38.92}\n",
            "Logging at step 16600: {'loss': 0.7082, 'learning_rate': 5.106132075471698e-06, 'epoch': 39.15}\n",
            "Logging at step 16700: {'loss': 0.7187, 'learning_rate': 5.076650943396226e-06, 'epoch': 39.39}\n",
            "Logging at step 16800: {'loss': 0.7256, 'learning_rate': 5.047169811320756e-06, 'epoch': 39.62}\n",
            "Logging at step 16900: {'loss': 0.6786, 'learning_rate': 5.017688679245284e-06, 'epoch': 39.86}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-17000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-17000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-17000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-17000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-17000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 17000: {'loss': 0.6624, 'learning_rate': 4.988207547169812e-06, 'epoch': 40.09}\n",
            "Logging at step 17100: {'loss': 0.6783, 'learning_rate': 4.958726415094339e-06, 'epoch': 40.33}\n",
            "Logging at step 17200: {'loss': 0.7699, 'learning_rate': 4.929245283018868e-06, 'epoch': 40.57}\n",
            "Logging at step 17300: {'loss': 0.6474, 'learning_rate': 4.899764150943397e-06, 'epoch': 40.8}\n",
            "Logging at step 17400: {'loss': 0.6554, 'learning_rate': 4.870283018867925e-06, 'epoch': 41.04}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-17500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-17500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-17500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-17500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-17500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 17500: {'loss': 0.679, 'learning_rate': 4.840801886792453e-06, 'epoch': 41.27}\n",
            "Logging at step 17600: {'loss': 0.6676, 'learning_rate': 4.811320754716982e-06, 'epoch': 41.51}\n",
            "Logging at step 17700: {'loss': 0.6912, 'learning_rate': 4.78183962264151e-06, 'epoch': 41.75}\n",
            "Logging at step 17800: {'loss': 0.7377, 'learning_rate': 4.752358490566038e-06, 'epoch': 41.98}\n",
            "Logging at step 17900: {'loss': 0.7136, 'learning_rate': 4.722877358490566e-06, 'epoch': 42.22}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-18000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-18000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-18000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-18000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-18000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 18000: {'loss': 0.6667, 'learning_rate': 4.693396226415095e-06, 'epoch': 42.45}\n",
            "Logging at step 18100: {'loss': 0.6741, 'learning_rate': 4.663915094339622e-06, 'epoch': 42.69}\n",
            "Logging at step 18200: {'loss': 0.7137, 'learning_rate': 4.634433962264152e-06, 'epoch': 42.92}\n",
            "Logging at step 18300: {'loss': 0.6799, 'learning_rate': 4.60495283018868e-06, 'epoch': 43.16}\n",
            "Logging at step 18400: {'loss': 0.6559, 'learning_rate': 4.575471698113208e-06, 'epoch': 43.4}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-18500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-18500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-18500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-18500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-18500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 18500: {'loss': 0.6856, 'learning_rate': 4.545990566037736e-06, 'epoch': 43.63}\n",
            "Logging at step 18600: {'loss': 0.7183, 'learning_rate': 4.516509433962265e-06, 'epoch': 43.87}\n",
            "Logging at step 18700: {'loss': 0.6591, 'learning_rate': 4.487028301886793e-06, 'epoch': 44.1}\n",
            "Logging at step 18800: {'loss': 0.697, 'learning_rate': 4.457547169811321e-06, 'epoch': 44.34}\n",
            "Logging at step 18900: {'loss': 0.6771, 'learning_rate': 4.428066037735849e-06, 'epoch': 44.58}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-19000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-19000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-19000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-19000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-19000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 19000: {'loss': 0.6769, 'learning_rate': 4.398584905660378e-06, 'epoch': 44.81}\n",
            "Logging at step 19100: {'loss': 0.6883, 'learning_rate': 4.369103773584906e-06, 'epoch': 45.05}\n",
            "Logging at step 19200: {'loss': 0.7012, 'learning_rate': 4.339622641509435e-06, 'epoch': 45.28}\n",
            "Logging at step 19300: {'loss': 0.6613, 'learning_rate': 4.310141509433963e-06, 'epoch': 45.52}\n",
            "Logging at step 19400: {'loss': 0.6952, 'learning_rate': 4.280660377358491e-06, 'epoch': 45.75}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-19500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-19500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-19500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-19500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-19500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 19500: {'loss': 0.6599, 'learning_rate': 4.251179245283019e-06, 'epoch': 45.99}\n",
            "Logging at step 19600: {'loss': 0.7476, 'learning_rate': 4.221698113207548e-06, 'epoch': 46.23}\n",
            "Logging at step 19700: {'loss': 0.662, 'learning_rate': 4.192216981132075e-06, 'epoch': 46.46}\n",
            "Logging at step 19800: {'loss': 0.6521, 'learning_rate': 4.162735849056604e-06, 'epoch': 46.7}\n",
            "Logging at step 19900: {'loss': 0.6841, 'learning_rate': 4.133254716981133e-06, 'epoch': 46.93}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-20000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-20000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-20000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-20000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-20000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 20000: {'loss': 0.6565, 'learning_rate': 4.103773584905661e-06, 'epoch': 47.17}\n",
            "Logging at step 20100: {'loss': 0.7277, 'learning_rate': 4.074292452830189e-06, 'epoch': 47.41}\n",
            "Logging at step 20200: {'loss': 0.6594, 'learning_rate': 4.0448113207547176e-06, 'epoch': 47.64}\n",
            "Logging at step 20300: {'loss': 0.6821, 'learning_rate': 4.015330188679246e-06, 'epoch': 47.88}\n",
            "Logging at step 20400: {'loss': 0.6805, 'learning_rate': 3.985849056603774e-06, 'epoch': 48.11}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-20500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-20500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-20500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-20500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-20500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 20500: {'loss': 0.6329, 'learning_rate': 3.956367924528302e-06, 'epoch': 48.35}\n",
            "Logging at step 20600: {'loss': 0.6567, 'learning_rate': 3.926886792452831e-06, 'epoch': 48.58}\n",
            "Logging at step 20700: {'loss': 0.69, 'learning_rate': 3.897405660377358e-06, 'epoch': 48.82}\n",
            "Logging at step 20800: {'loss': 0.6704, 'learning_rate': 3.8679245283018875e-06, 'epoch': 49.06}\n",
            "Logging at step 20900: {'loss': 0.652, 'learning_rate': 3.838443396226416e-06, 'epoch': 49.29}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-21000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-21000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-21000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-21000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-21000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 21000: {'loss': 0.6662, 'learning_rate': 3.808962264150944e-06, 'epoch': 49.53}\n",
            "Logging at step 21100: {'loss': 0.6806, 'learning_rate': 3.779481132075472e-06, 'epoch': 49.76}\n",
            "Logging at step 21200: {'loss': 0.6903, 'learning_rate': 3.7500000000000005e-06, 'epoch': 50.0}\n",
            "Logging at step 21300: {'loss': 0.6773, 'learning_rate': 3.7205188679245286e-06, 'epoch': 50.24}\n",
            "Logging at step 21400: {'loss': 0.6351, 'learning_rate': 3.6910377358490566e-06, 'epoch': 50.47}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-21500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-21500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-21500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-21500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-21500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 21500: {'loss': 0.7035, 'learning_rate': 3.661556603773585e-06, 'epoch': 50.71}\n",
            "Logging at step 21600: {'loss': 0.6849, 'learning_rate': 3.632075471698113e-06, 'epoch': 50.94}\n",
            "Logging at step 21700: {'loss': 0.6265, 'learning_rate': 3.602594339622642e-06, 'epoch': 51.18}\n",
            "Logging at step 21800: {'loss': 0.6863, 'learning_rate': 3.5731132075471705e-06, 'epoch': 51.42}\n",
            "Logging at step 21900: {'loss': 0.6896, 'learning_rate': 3.5436320754716985e-06, 'epoch': 51.65}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-22000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-22000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-22000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-22000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-22000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 22000: {'loss': 0.6657, 'learning_rate': 3.514150943396227e-06, 'epoch': 51.89}\n",
            "Logging at step 22100: {'loss': 0.6376, 'learning_rate': 3.484669811320755e-06, 'epoch': 52.12}\n",
            "Logging at step 22200: {'loss': 0.6709, 'learning_rate': 3.455188679245283e-06, 'epoch': 52.36}\n",
            "Logging at step 22300: {'loss': 0.656, 'learning_rate': 3.4257075471698115e-06, 'epoch': 52.59}\n",
            "Logging at step 22400: {'loss': 0.6989, 'learning_rate': 3.3962264150943395e-06, 'epoch': 52.83}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-22500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-22500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-22500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-22500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-22500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 22500: {'loss': 0.6396, 'learning_rate': 3.366745283018868e-06, 'epoch': 53.07}\n",
            "Logging at step 22600: {'loss': 0.6666, 'learning_rate': 3.337264150943397e-06, 'epoch': 53.3}\n",
            "Logging at step 22700: {'loss': 0.6384, 'learning_rate': 3.307783018867925e-06, 'epoch': 53.54}\n",
            "Logging at step 22800: {'loss': 0.6334, 'learning_rate': 3.2783018867924534e-06, 'epoch': 53.77}\n",
            "Logging at step 22900: {'loss': 0.7239, 'learning_rate': 3.2488207547169814e-06, 'epoch': 54.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-23000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-23000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-23000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-23000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-23000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 23000: {'loss': 0.645, 'learning_rate': 3.2193396226415095e-06, 'epoch': 54.25}\n",
            "Logging at step 23100: {'loss': 0.6615, 'learning_rate': 3.189858490566038e-06, 'epoch': 54.48}\n",
            "Logging at step 23200: {'loss': 0.6539, 'learning_rate': 3.160377358490566e-06, 'epoch': 54.72}\n",
            "Logging at step 23300: {'loss': 0.6592, 'learning_rate': 3.1308962264150945e-06, 'epoch': 54.95}\n",
            "Logging at step 23400: {'loss': 0.7105, 'learning_rate': 3.1014150943396225e-06, 'epoch': 55.19}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-23500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-23500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-23500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-23500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-23500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 23500: {'loss': 0.619, 'learning_rate': 3.0719339622641514e-06, 'epoch': 55.42}\n",
            "Logging at step 23600: {'loss': 0.6891, 'learning_rate': 3.04245283018868e-06, 'epoch': 55.66}\n",
            "Logging at step 23700: {'loss': 0.643, 'learning_rate': 3.012971698113208e-06, 'epoch': 55.9}\n",
            "Logging at step 23800: {'loss': 0.6381, 'learning_rate': 2.983490566037736e-06, 'epoch': 56.13}\n",
            "Logging at step 23900: {'loss': 0.6791, 'learning_rate': 2.9540094339622644e-06, 'epoch': 56.37}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-24000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-24000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-24000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-24000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-24000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 24000: {'loss': 0.6827, 'learning_rate': 2.9245283018867924e-06, 'epoch': 56.6}\n",
            "Logging at step 24100: {'loss': 0.6329, 'learning_rate': 2.895047169811321e-06, 'epoch': 56.84}\n",
            "Logging at step 24200: {'loss': 0.6419, 'learning_rate': 2.865566037735849e-06, 'epoch': 57.08}\n",
            "Logging at step 24300: {'loss': 0.641, 'learning_rate': 2.836084905660378e-06, 'epoch': 57.31}\n",
            "Logging at step 24400: {'loss': 0.6775, 'learning_rate': 2.8066037735849063e-06, 'epoch': 57.55}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-24500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-24500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-24500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-24500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-24500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 24500: {'loss': 0.6884, 'learning_rate': 2.7771226415094343e-06, 'epoch': 57.78}\n",
            "Logging at step 24600: {'loss': 0.6321, 'learning_rate': 2.7476415094339624e-06, 'epoch': 58.02}\n",
            "Logging at step 24700: {'loss': 0.6913, 'learning_rate': 2.718160377358491e-06, 'epoch': 58.25}\n",
            "Logging at step 24800: {'loss': 0.6276, 'learning_rate': 2.688679245283019e-06, 'epoch': 58.49}\n",
            "Logging at step 24900: {'loss': 0.6567, 'learning_rate': 2.6591981132075473e-06, 'epoch': 58.73}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-25000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-25000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-25000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-25000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-25000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 25000: {'loss': 0.6488, 'learning_rate': 2.6297169811320754e-06, 'epoch': 58.96}\n",
            "Logging at step 25100: {'loss': 0.645, 'learning_rate': 2.600235849056604e-06, 'epoch': 59.2}\n",
            "Logging at step 25200: {'loss': 0.6801, 'learning_rate': 2.5707547169811327e-06, 'epoch': 59.43}\n",
            "Logging at step 25300: {'loss': 0.6693, 'learning_rate': 2.5412735849056608e-06, 'epoch': 59.67}\n",
            "Logging at step 25400: {'loss': 0.621, 'learning_rate': 2.511792452830189e-06, 'epoch': 59.91}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-25500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-25500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-25500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-25500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-25500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 25500: {'loss': 0.6274, 'learning_rate': 2.4823113207547173e-06, 'epoch': 60.14}\n",
            "Logging at step 25600: {'loss': 0.7023, 'learning_rate': 2.4528301886792453e-06, 'epoch': 60.38}\n",
            "Logging at step 25700: {'loss': 0.673, 'learning_rate': 2.4233490566037738e-06, 'epoch': 60.61}\n",
            "Logging at step 25800: {'loss': 0.6436, 'learning_rate': 2.393867924528302e-06, 'epoch': 60.85}\n",
            "Logging at step 25900: {'loss': 0.6288, 'learning_rate': 2.3643867924528303e-06, 'epoch': 61.08}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-26000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-26000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-26000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-26000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-26000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 26000: {'loss': 0.6745, 'learning_rate': 2.3349056603773588e-06, 'epoch': 61.32}\n",
            "Logging at step 26100: {'loss': 0.6157, 'learning_rate': 2.305424528301887e-06, 'epoch': 61.56}\n",
            "Logging at step 26200: {'loss': 0.6509, 'learning_rate': 2.2759433962264153e-06, 'epoch': 61.79}\n",
            "Logging at step 26300: {'loss': 0.6825, 'learning_rate': 2.2464622641509437e-06, 'epoch': 62.03}\n",
            "Logging at step 26400: {'loss': 0.6369, 'learning_rate': 2.2169811320754718e-06, 'epoch': 62.26}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-26500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-26500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-26500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-26500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-26500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 26500: {'loss': 0.704, 'learning_rate': 2.1875000000000002e-06, 'epoch': 62.5}\n",
            "Logging at step 26600: {'loss': 0.6127, 'learning_rate': 2.1580188679245283e-06, 'epoch': 62.74}\n",
            "Logging at step 26700: {'loss': 0.6616, 'learning_rate': 2.1285377358490567e-06, 'epoch': 62.97}\n",
            "Logging at step 26800: {'loss': 0.6389, 'learning_rate': 2.099056603773585e-06, 'epoch': 63.21}\n",
            "Logging at step 26900: {'loss': 0.6369, 'learning_rate': 2.0695754716981132e-06, 'epoch': 63.44}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-27000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-27000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-27000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-27000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-27000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 27000: {'loss': 0.6869, 'learning_rate': 2.0400943396226417e-06, 'epoch': 63.68}\n",
            "Logging at step 27100: {'loss': 0.6363, 'learning_rate': 2.0106132075471697e-06, 'epoch': 63.92}\n",
            "Logging at step 27200: {'loss': 0.678, 'learning_rate': 1.981132075471698e-06, 'epoch': 64.15}\n",
            "Logging at step 27300: {'loss': 0.6424, 'learning_rate': 1.9516509433962267e-06, 'epoch': 64.39}\n",
            "Logging at step 27400: {'loss': 0.6169, 'learning_rate': 1.9221698113207547e-06, 'epoch': 64.62}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-27500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-27500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-27500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-27500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-27500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 27500: {'loss': 0.6813, 'learning_rate': 1.8926886792452832e-06, 'epoch': 64.86}\n",
            "Logging at step 27600: {'loss': 0.6291, 'learning_rate': 1.8632075471698114e-06, 'epoch': 65.09}\n",
            "Logging at step 27700: {'loss': 0.6508, 'learning_rate': 1.83372641509434e-06, 'epoch': 65.33}\n",
            "Logging at step 27800: {'loss': 0.6414, 'learning_rate': 1.8042452830188682e-06, 'epoch': 65.57}\n",
            "Logging at step 27900: {'loss': 0.6738, 'learning_rate': 1.7747641509433964e-06, 'epoch': 65.8}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-28000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-28000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-28000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-28000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-28000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 28000: {'loss': 0.6358, 'learning_rate': 1.7452830188679247e-06, 'epoch': 66.04}\n",
            "Logging at step 28100: {'loss': 0.6944, 'learning_rate': 1.7158018867924531e-06, 'epoch': 66.27}\n",
            "Logging at step 28200: {'loss': 0.6459, 'learning_rate': 1.6863207547169814e-06, 'epoch': 66.51}\n",
            "Logging at step 28300: {'loss': 0.6313, 'learning_rate': 1.6568396226415096e-06, 'epoch': 66.75}\n",
            "Logging at step 28400: {'loss': 0.6459, 'learning_rate': 1.6273584905660379e-06, 'epoch': 66.98}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-28500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-28500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-28500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-28500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-28500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 28500: {'loss': 0.5868, 'learning_rate': 1.5978773584905663e-06, 'epoch': 67.22}\n",
            "Logging at step 28600: {'loss': 0.6137, 'learning_rate': 1.5683962264150946e-06, 'epoch': 67.45}\n",
            "Logging at step 28700: {'loss': 0.7055, 'learning_rate': 1.5389150943396228e-06, 'epoch': 67.69}\n",
            "Logging at step 28800: {'loss': 0.675, 'learning_rate': 1.509433962264151e-06, 'epoch': 67.92}\n",
            "Logging at step 28900: {'loss': 0.6853, 'learning_rate': 1.4799528301886794e-06, 'epoch': 68.16}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-29000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-29000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-29000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-29000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-29000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 29000: {'loss': 0.718, 'learning_rate': 1.4504716981132078e-06, 'epoch': 68.4}\n",
            "Logging at step 29100: {'loss': 0.6183, 'learning_rate': 1.420990566037736e-06, 'epoch': 68.63}\n",
            "Logging at step 29200: {'loss': 0.597, 'learning_rate': 1.3915094339622643e-06, 'epoch': 68.87}\n",
            "Logging at step 29300: {'loss': 0.6153, 'learning_rate': 1.3620283018867926e-06, 'epoch': 69.1}\n",
            "Logging at step 29400: {'loss': 0.5839, 'learning_rate': 1.332547169811321e-06, 'epoch': 69.34}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-29500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-29500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-29500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-29500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-29500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 29500: {'loss': 0.5944, 'learning_rate': 1.3030660377358493e-06, 'epoch': 69.58}\n",
            "Logging at step 29600: {'loss': 0.725, 'learning_rate': 1.2735849056603775e-06, 'epoch': 69.81}\n",
            "Logging at step 29700: {'loss': 0.6724, 'learning_rate': 1.2441037735849058e-06, 'epoch': 70.05}\n",
            "Logging at step 29800: {'loss': 0.6456, 'learning_rate': 1.214622641509434e-06, 'epoch': 70.28}\n",
            "Logging at step 29900: {'loss': 0.6049, 'learning_rate': 1.1851415094339623e-06, 'epoch': 70.52}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-30000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-30000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-30000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-30000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-30000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 30000: {'loss': 0.7109, 'learning_rate': 1.1556603773584908e-06, 'epoch': 70.75}\n",
            "Logging at step 30100: {'loss': 0.6432, 'learning_rate': 1.126179245283019e-06, 'epoch': 70.99}\n",
            "Logging at step 30200: {'loss': 0.6313, 'learning_rate': 1.0966981132075473e-06, 'epoch': 71.23}\n",
            "Logging at step 30300: {'loss': 0.6341, 'learning_rate': 1.0672169811320755e-06, 'epoch': 71.46}\n",
            "Logging at step 30400: {'loss': 0.6646, 'learning_rate': 1.037735849056604e-06, 'epoch': 71.7}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-30500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-30500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-30500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-30500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-30500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 30500: {'loss': 0.6201, 'learning_rate': 1.0082547169811322e-06, 'epoch': 71.93}\n",
            "Logging at step 30600: {'loss': 0.6808, 'learning_rate': 9.787735849056605e-07, 'epoch': 72.17}\n",
            "Logging at step 30700: {'loss': 0.6655, 'learning_rate': 9.492924528301887e-07, 'epoch': 72.41}\n",
            "Logging at step 30800: {'loss': 0.5836, 'learning_rate': 9.19811320754717e-07, 'epoch': 72.64}\n",
            "Logging at step 30900: {'loss': 0.6432, 'learning_rate': 8.903301886792454e-07, 'epoch': 72.88}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-31000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-31000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-31000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-31000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-31000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 31000: {'loss': 0.6476, 'learning_rate': 8.608490566037736e-07, 'epoch': 73.11}\n",
            "Logging at step 31100: {'loss': 0.6566, 'learning_rate': 8.31367924528302e-07, 'epoch': 73.35}\n",
            "Logging at step 31200: {'loss': 0.6573, 'learning_rate': 8.018867924528302e-07, 'epoch': 73.58}\n",
            "Logging at step 31300: {'loss': 0.6104, 'learning_rate': 7.724056603773586e-07, 'epoch': 73.82}\n",
            "Logging at step 31400: {'loss': 0.639, 'learning_rate': 7.429245283018868e-07, 'epoch': 74.06}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-31500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-31500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-31500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-31500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-31500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 31500: {'loss': 0.617, 'learning_rate': 7.134433962264152e-07, 'epoch': 74.29}\n",
            "Logging at step 31600: {'loss': 0.6519, 'learning_rate': 6.839622641509434e-07, 'epoch': 74.53}\n",
            "Logging at step 31700: {'loss': 0.6556, 'learning_rate': 6.544811320754718e-07, 'epoch': 74.76}\n",
            "Logging at step 31800: {'loss': 0.6519, 'learning_rate': 6.25e-07, 'epoch': 75.0}\n",
            "Logging at step 31900: {'loss': 0.6283, 'learning_rate': 5.955188679245284e-07, 'epoch': 75.24}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-32000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-32000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-32000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-32000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-32000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 32000: {'loss': 0.5938, 'learning_rate': 5.660377358490567e-07, 'epoch': 75.47}\n",
            "Logging at step 32100: {'loss': 0.6769, 'learning_rate': 5.36556603773585e-07, 'epoch': 75.71}\n",
            "Logging at step 32200: {'loss': 0.6487, 'learning_rate': 5.070754716981133e-07, 'epoch': 75.94}\n",
            "Logging at step 32300: {'loss': 0.6517, 'learning_rate': 4.775943396226416e-07, 'epoch': 76.18}\n",
            "Logging at step 32400: {'loss': 0.6422, 'learning_rate': 4.4811320754716983e-07, 'epoch': 76.42}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-32500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-32500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-32500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-32500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-32500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 32500: {'loss': 0.5898, 'learning_rate': 4.1863207547169814e-07, 'epoch': 76.65}\n",
            "Logging at step 32600: {'loss': 0.662, 'learning_rate': 3.8915094339622644e-07, 'epoch': 76.89}\n",
            "Logging at step 32700: {'loss': 0.6361, 'learning_rate': 3.5966981132075475e-07, 'epoch': 77.12}\n",
            "Logging at step 32800: {'loss': 0.613, 'learning_rate': 3.3018867924528305e-07, 'epoch': 77.36}\n",
            "Logging at step 32900: {'loss': 0.6295, 'learning_rate': 3.0070754716981136e-07, 'epoch': 77.59}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-33000\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-33000/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-33000/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-33000/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-33000/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 33000: {'loss': 0.6275, 'learning_rate': 2.7122641509433966e-07, 'epoch': 77.83}\n",
            "Logging at step 33100: {'loss': 0.6968, 'learning_rate': 2.4174528301886797e-07, 'epoch': 78.07}\n",
            "Logging at step 33200: {'loss': 0.6237, 'learning_rate': 2.1226415094339622e-07, 'epoch': 78.3}\n",
            "Logging at step 33300: {'loss': 0.624, 'learning_rate': 1.8278301886792453e-07, 'epoch': 78.54}\n",
            "Logging at step 33400: {'loss': 0.6332, 'learning_rate': 1.5330188679245283e-07, 'epoch': 78.77}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-33500\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-33500/sst/adapter_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-33500/sst/pytorch_adapter.bin\n",
            "Configuration saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-33500/sst/head_config.json\n",
            "Module weights saved in ./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-33500/sst/pytorch_model_head.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 33500: {'loss': 0.6577, 'learning_rate': 1.2382075471698114e-07, 'epoch': 79.01}\n",
            "Logging at step 33600: {'loss': 0.6244, 'learning_rate': 9.433962264150944e-08, 'epoch': 79.25}\n",
            "Logging at step 33700: {'loss': 0.6646, 'learning_rate': 6.485849056603775e-08, 'epoch': 79.48}\n",
            "Logging at step 33800: {'loss': 0.6324, 'learning_rate': 3.537735849056604e-08, 'epoch': 79.72}\n",
            "Logging at step 33900: {'loss': 0.6542, 'learning_rate': 5.89622641509434e-09, 'epoch': 79.95}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging at step 33920: {'train_runtime': 4495.5097, 'train_samples_per_second': 60.327, 'train_steps_per_second': 7.545, 'total_flos': 1.8307213045056e+16, 'train_loss': 0.723526682943668, 'epoch': 80.0}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=33920, training_loss=0.723526682943668, metrics={'train_runtime': 4495.5097, 'train_samples_per_second': 60.327, 'train_steps_per_second': 7.545, 'total_flos': 1.8307213045056e+16, 'train_loss': 0.723526682943668, 'epoch': 80.0})"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRAvA7-JZqEg"
      },
      "source": [
        "Interpreting the **loss**:\n",
        "\n",
        "the loss is the \"error\" that our classifier makes when guessing whether a sentence is \"positive\" or \"negative\". As you can see, expectedly, loss decreases during training."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0I9oeO_Ug5oa",
        "outputId": "4bc7eab4-bf3f-4e95-c9b9-f4111decb4a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Shareddrives/Computational Semantics/Assignmen 3/Graded part/Scripts/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## W load the saved checkpoint to evaluate the model"
      ],
      "metadata": {
        "id": "Ic7gziN8zpsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/MyDrive' #define your path"
      ],
      "metadata": {
        "id": "2_0sSD2F0BJZ",
        "outputId": "23815e0b-202a-46b8-febc-5479319e2aae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[Errno 2] No such file or directory: '/content/drive/MyDrive #define your path'\n",
            "/content/drive/Shareddrives/Computational Semantics/Assignmen 3/Graded part/Scripts/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the base model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\", config=config)\n",
        "\n",
        "# Load the adapter from the checkpoint\n",
        "adapter_name = model.load_adapter(\n",
        "    './training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-29500/sst/',\n",
        "    config='./training_output_BERTtweet_clean_input_bert_80_epochs/checkpoint-29500/sst/adapter_config.json',\n",
        "    load_as='sst',  # Name of the adapter\n",
        "    local_files_only=True,  # Load from local files\n",
        "    id2label={0: \"comment\", 1: \"support\", 2: 'deny', 3: \"query\"}\n",
        ")\n",
        "\n",
        "# Set active adapters\n",
        "model.set_active_adapters(adapter_name)\n",
        "\n",
        "# Configure the model to update only the adapter parameters during training\n",
        "model.train_adapter(adapter_name)\n",
        "\n",
        "# Update training arguments for additional epochs\n",
        "training_args.num_train_epochs = 200  # Updated to 200 epochs\n",
        "\n",
        "# Re-instantiate the Trainer\n",
        "trainer = AdapterTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        "    compute_metrics=compute_accuracy,\n",
        "    callbacks=[loss_logging_callback]\n",
        ")\n",
        "\n",
        "# Resume training\n",
        "#trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "EQe_YWnOvrmj",
        "outputId": "d03bf223-5fa7-4eb0-ddac-fd6d125c0e4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d21b110e0c04>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the base model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vinai/bertweet-base\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the adapter from the checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m adapter_name = model.load_adapter(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForSequenceClassification' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4Zfm9o-jnxK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "1cfcebe2-84e1-4f34-8cd6-8072e2e7facb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.bar(range(len(loss_logging_callback.losses)), loss_logging_callback.losses)\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss per Logging Step')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uqd-LoslUnR"
      },
      "source": [
        "## 3. Evaluation and error analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGpIdkuopXFH"
      },
      "source": [
        "We have used our **validation set** to check the model's performances during training. General result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JW6g0yF2NeT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "7724d13d-2e80-4596-f2b0-d151fd354d81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 281\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [36/36 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7259786476868327\n",
            "Logging at step 0: {'eval_acc': 0.7259786476868327, 'eval_loss': 0.8467274904251099, 'eval_runtime': 2.5772, 'eval_samples_per_second': 109.033, 'eval_steps_per_second': 13.969}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_acc': 0.7259786476868327,\n",
              " 'eval_loss': 0.8467274904251099,\n",
              " 'eval_runtime': 2.5772,\n",
              " 'eval_samples_per_second': 109.033,\n",
              " 'eval_steps_per_second': 13.969}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3p4eA71PWXm"
      },
      "source": [
        "The number that you see after **\"eval_acc\"** is the accuracy of the model on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiuWuQYr2Ni4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ff6d5b8d-baba-4a76-e943-6099b262efd0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'query'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "# Creating a text classification pipeline, using our model (with the trained adapter added)\n",
        "classifier = TextClassificationPipeline(model=model, # The model to use for classification\n",
        "                                        tokenizer=tokenizer, # The tokenizer to process the input text\n",
        "                                        device=training_args.device.index # The device (CPU/GPU) to run the model on\n",
        "                                        )\n",
        "\n",
        "# @title Model prediction\n",
        "classifier(\"do you know if this is true?\")[0]['label'] # Use the classifier to predict the label of the given text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhuxtc0BiNAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dde66543-1167-4699-de56-07fc65916ae6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9435333609580994"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "# @title How confident is our model with its prediction:\n",
        "classifier(\"do you know if this is true?\")[0]['score']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y38Ry8gEZ5yA"
      },
      "source": [
        "We are satisfied with our model's performances. **GB**: but we don't have the performance, right? only the confidence for one datapoint?\n",
        "\n",
        "We now inspect the model's errors on our **test set**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHL90XO62NoN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8de2ac4-9ed1-488b-b17d-00bd16e9ac0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1048: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     comment       0.78      0.84      0.81       173\n",
            "     support       0.62      0.55      0.58        69\n",
            "        deny       0.20      0.09      0.13        11\n",
            "       query       0.70      0.68      0.69        28\n",
            "\n",
            "    accuracy                           0.73       281\n",
            "   macro avg       0.58      0.54      0.55       281\n",
            "weighted avg       0.71      0.73      0.72       281\n",
            "\n",
            "[[146  19   2   6]\n",
            " [ 28  38   2   1]\n",
            " [  5   4   1   1]\n",
            " [  9   0   0  19]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Assuming your classifier function outputs labels directly as 'comment', 'support', 'deny', 'query'\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "for text, label in list(zip(df_test['input'], df_test['output'])):  # a subset for testing\n",
        "    prediction = classifier(text)[0]['label']\n",
        "    true_labels.append(label)\n",
        "    pred_labels.append(prediction)\n",
        "\n",
        "# Now, convert these labels into numeric form for sklearn functions\n",
        "label_mapping = {'comment': 0, 'support': 1, 'deny': 2, 'query': 3}\n",
        "numeric_true_labels = [label_mapping[label] for label in true_labels]\n",
        "numeric_pred_labels = [label_mapping[label] for label in pred_labels]\n",
        "\n",
        "# Generate a classification report\n",
        "print(classification_report(numeric_true_labels, numeric_pred_labels, target_names=label_mapping.keys()))\n",
        "\n",
        "# Generate a confusion matrix\n",
        "conf_matrix = confusion_matrix(numeric_true_labels, numeric_pred_labels)\n",
        "print(conf_matrix)\n",
        "temp_dic = {'text': df_test['input'],\n",
        "            'true': true_labels,\n",
        "            'predicted': pred_labels}\n",
        "\n",
        "true_pred_df = pd.DataFrame(temp_dic)\n",
        "true_pred_df.to_csv(\"./tweets_cleaned_adapter_2_BERTtweet_clean_100e.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6357789bfe1c4603ab378c7aa3fc813e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_af008c8cc7e6429f89062fdfc23604ab",
              "IPY_MODEL_cbd4a6dc07cb498abb1dce78e6f83ebb",
              "IPY_MODEL_7599bde5999e44eea1b9d481daaf59f0"
            ],
            "layout": "IPY_MODEL_cd7430b9b9fb4f298a3eb54fb36e77f2"
          }
        },
        "af008c8cc7e6429f89062fdfc23604ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6aa03249330f4ac88e905dcbaef56afb",
            "placeholder": "​",
            "style": "IPY_MODEL_c250c89260b946579b805c726a84513b",
            "value": "vocab.txt: 100%"
          }
        },
        "cbd4a6dc07cb498abb1dce78e6f83ebb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_260c26a28a914361bfac3fe297a28338",
            "max": 843438,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cb16cbb90c36423f877dcf2d2eb0f416",
            "value": 843438
          }
        },
        "7599bde5999e44eea1b9d481daaf59f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10b59045e2cc4b80970a3fff1338911b",
            "placeholder": "​",
            "style": "IPY_MODEL_7d59387fd9f5409daf975079e15325bc",
            "value": " 843k/843k [00:00&lt;00:00, 3.20MB/s]"
          }
        },
        "cd7430b9b9fb4f298a3eb54fb36e77f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6aa03249330f4ac88e905dcbaef56afb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c250c89260b946579b805c726a84513b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "260c26a28a914361bfac3fe297a28338": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb16cbb90c36423f877dcf2d2eb0f416": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "10b59045e2cc4b80970a3fff1338911b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d59387fd9f5409daf975079e15325bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99dd8548fc3d4616bb434f4f7efdc5e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32e3db7e36b9466590253acc8ef4ba45",
              "IPY_MODEL_dbb14678393a4495a6286b24f37531f6",
              "IPY_MODEL_1d32bbded75e47bb96789670e524d8ff"
            ],
            "layout": "IPY_MODEL_be19a58554014e8f938da4776af6d3d6"
          }
        },
        "32e3db7e36b9466590253acc8ef4ba45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36a490f525a54034963d4ae63182ad5b",
            "placeholder": "​",
            "style": "IPY_MODEL_269ad6d98bab42bb86ff51f9b59ae578",
            "value": "config.json: 100%"
          }
        },
        "dbb14678393a4495a6286b24f37531f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee611c4a852a46cd983f530db58916a1",
            "max": 558,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_60da43d24f3c4eed9d6559aa2eb30c15",
            "value": 558
          }
        },
        "1d32bbded75e47bb96789670e524d8ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65315d6c6dc44c27aad93e0dac18c196",
            "placeholder": "​",
            "style": "IPY_MODEL_37179af5f9404a5f9ec7a0a15209f5a5",
            "value": " 558/558 [00:00&lt;00:00, 36.2kB/s]"
          }
        },
        "be19a58554014e8f938da4776af6d3d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36a490f525a54034963d4ae63182ad5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "269ad6d98bab42bb86ff51f9b59ae578": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee611c4a852a46cd983f530db58916a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60da43d24f3c4eed9d6559aa2eb30c15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65315d6c6dc44c27aad93e0dac18c196": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37179af5f9404a5f9ec7a0a15209f5a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f177af253d341d8afe8fda96f5c305e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a3dbe0afd3b942bfbafe747ea09805df",
              "IPY_MODEL_2e71672af6ef449a9ff3881dc83d6412",
              "IPY_MODEL_628e3504f1e249509b979c08b6e2ab84"
            ],
            "layout": "IPY_MODEL_db935c1565ec498bb8167fd9eba8a26d"
          }
        },
        "a3dbe0afd3b942bfbafe747ea09805df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfe863c8e37b4dffa0c464c1f16133d6",
            "placeholder": "​",
            "style": "IPY_MODEL_72d1c90f8cee4ed881ee898401caf178",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "2e71672af6ef449a9ff3881dc83d6412": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a8569324b8348dd8f81dfad2750ea9e",
            "max": 542529064,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23d70222df9543ea94a2601f748375c4",
            "value": 542529064
          }
        },
        "628e3504f1e249509b979c08b6e2ab84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e18c0cbd5cc45f58f8a22bad0b176ce",
            "placeholder": "​",
            "style": "IPY_MODEL_a035eb6f0dbf4083a1c1890dd6019a51",
            "value": " 543M/543M [00:02&lt;00:00, 204MB/s]"
          }
        },
        "db935c1565ec498bb8167fd9eba8a26d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfe863c8e37b4dffa0c464c1f16133d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72d1c90f8cee4ed881ee898401caf178": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a8569324b8348dd8f81dfad2750ea9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23d70222df9543ea94a2601f748375c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e18c0cbd5cc45f58f8a22bad0b176ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a035eb6f0dbf4083a1c1890dd6019a51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eaa65468c3ee4298b860fbaf984cb6d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_295da6388a74422d93448666f95ac89d",
              "IPY_MODEL_2fd6ce67797e409491a8655b35756bb9",
              "IPY_MODEL_08b26fad036a455496a409b8fc1a714c"
            ],
            "layout": "IPY_MODEL_247868c6fb0e49e281030722ff323c54"
          }
        },
        "295da6388a74422d93448666f95ac89d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70adc9a911cc42de87fb6fd1abcd738a",
            "placeholder": "​",
            "style": "IPY_MODEL_ac2414d9a7de4d4c891e9d22831751ca",
            "value": "Map: 100%"
          }
        },
        "2fd6ce67797e409491a8655b35756bb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28ffc1ec9dbc4936a33fc53e2a4e23a1",
            "max": 3390,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ef6abe308c6467db50ba964e00ec3c7",
            "value": 3390
          }
        },
        "08b26fad036a455496a409b8fc1a714c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15ddbf07fa384f4183481dec1ac28297",
            "placeholder": "​",
            "style": "IPY_MODEL_c496a74b74f04ba8908d88d9bb2a00f5",
            "value": " 3390/3390 [00:00&lt;00:00, 4649.86 examples/s]"
          }
        },
        "247868c6fb0e49e281030722ff323c54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70adc9a911cc42de87fb6fd1abcd738a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac2414d9a7de4d4c891e9d22831751ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28ffc1ec9dbc4936a33fc53e2a4e23a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ef6abe308c6467db50ba964e00ec3c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "15ddbf07fa384f4183481dec1ac28297": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c496a74b74f04ba8908d88d9bb2a00f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8080e3ac1f9b4769a513d97dfb8d0e6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ec1de94c2ad24b6da0ab82f4dc6173cf",
              "IPY_MODEL_a532669b63bc4a0299ccc31ad0f77112",
              "IPY_MODEL_4a5b837b4157417eb68c945500e6e770"
            ],
            "layout": "IPY_MODEL_0d5cf14855934763b8f156acd7931473"
          }
        },
        "ec1de94c2ad24b6da0ab82f4dc6173cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2872eef708ec4bbeb89f5abd29672d9a",
            "placeholder": "​",
            "style": "IPY_MODEL_8fa19eef8ef5438eaa2222a187baf315",
            "value": "Map: 100%"
          }
        },
        "a532669b63bc4a0299ccc31ad0f77112": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb7f7cbdc53640c2bf72e2055269f2d7",
            "max": 281,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_895671f1518940d19558f14d00b9350e",
            "value": 281
          }
        },
        "4a5b837b4157417eb68c945500e6e770": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f8624e134ed4816be008f28995baf67",
            "placeholder": "​",
            "style": "IPY_MODEL_08ef934da68b4290b2095f38dfa21b99",
            "value": " 281/281 [00:00&lt;00:00, 2801.58 examples/s]"
          }
        },
        "0d5cf14855934763b8f156acd7931473": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2872eef708ec4bbeb89f5abd29672d9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fa19eef8ef5438eaa2222a187baf315": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb7f7cbdc53640c2bf72e2055269f2d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "895671f1518940d19558f14d00b9350e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f8624e134ed4816be008f28995baf67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08ef934da68b4290b2095f38dfa21b99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c97c91d1eba24ec5bb1bf742749112ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_17f647db2d294704986c298fc119b492",
              "IPY_MODEL_754726761de8491ba22aa404f2f20e92",
              "IPY_MODEL_af3c9243fdf7493bb4d7b960fdf628c4"
            ],
            "layout": "IPY_MODEL_f73b66ad25c2445e92f6c1fc492635a5"
          }
        },
        "17f647db2d294704986c298fc119b492": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c6db585a9be4721911745b8ff6400c6",
            "placeholder": "​",
            "style": "IPY_MODEL_5b6468a67c0d41f7acddac8a17498cf9",
            "value": "Map: 100%"
          }
        },
        "754726761de8491ba22aa404f2f20e92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54fe5231124d41bca3590d49e46e7e38",
            "max": 281,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6102c32b0e6d4af584a398312e19d297",
            "value": 281
          }
        },
        "af3c9243fdf7493bb4d7b960fdf628c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3dc8f9e89d51473d8060b821a8178468",
            "placeholder": "​",
            "style": "IPY_MODEL_62b5bee40af542c296875f76f5e64278",
            "value": " 281/281 [00:00&lt;00:00, 2881.51 examples/s]"
          }
        },
        "f73b66ad25c2445e92f6c1fc492635a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c6db585a9be4721911745b8ff6400c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b6468a67c0d41f7acddac8a17498cf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54fe5231124d41bca3590d49e46e7e38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6102c32b0e6d4af584a398312e19d297": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3dc8f9e89d51473d8060b821a8178468": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62b5bee40af542c296875f76f5e64278": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}